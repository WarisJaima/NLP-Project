{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Filtered Samples: 419\n",
      "\n",
      "🚀 Training for: INTRO\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b8d507db964facb59e85aefc5448cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dbbc6687dc41c89c86307dcdc7ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 03:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.156000</td>\n",
       "      <td>2.705085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.897300</td>\n",
       "      <td>2.639382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.804200</td>\n",
       "      <td>2.607487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.646500</td>\n",
       "      <td>2.593783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ./t5_intro_lora_v23\n",
      "\n",
      "🚀 Training for: BODY1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd37b9054ef441a191e6605c66db735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca0405ceee64e55967b6e3a92ea866d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 03:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.530800</td>\n",
       "      <td>3.342158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.447700</td>\n",
       "      <td>3.281767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.345900</td>\n",
       "      <td>3.261563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.239500</td>\n",
       "      <td>3.253983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ./t5_body1_lora_v23\n",
      "\n",
      "🚀 Training for: BODY2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52f2490d8e9436ea237478bb1179fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952a2f720c9a4432901cb58f036c338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 04:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.368400</td>\n",
       "      <td>3.230798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.334200</td>\n",
       "      <td>3.181254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.321200</td>\n",
       "      <td>3.161742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.033800</td>\n",
       "      <td>3.150816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ./t5_body2_lora_v23\n",
      "\n",
      "🚀 Training for: CONCLUSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf031fb6c6c4f53a960fc8e5bb48096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025b283821ec4424a7a342d7cbf09b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 03:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.155500</td>\n",
       "      <td>2.887333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.110800</td>\n",
       "      <td>2.841069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.986600</td>\n",
       "      <td>2.824035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.899600</td>\n",
       "      <td>2.816853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model to: ./t5_conclusion_lora_v23\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1 (v23b - + Argument Generation Loss) ===\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, pipeline\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from random import sample, random\n",
    "from transformers import AutoTokenizer as AutoTokenizerNLI, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer and paraphraser globally\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"ramsrigouthamg/t5_paraphraser\")\n",
    "\n",
    "# Load NLI model\n",
    "nli_tokenizer = AutoTokenizerNLI.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\").eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_contradiction_loss(premises, hypotheses):\n",
    "    losses = []\n",
    "    for premise, hypo in zip(premises, hypotheses):\n",
    "        inputs = nli_tokenizer(premise, hypo, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = nli_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        contradiction_prob = probs[:, 2]  # label 2 = contradiction\n",
    "        loss = 1.0 - contradiction_prob.mean()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def topic_relevance_loss(topics, generations):\n",
    "    losses = []\n",
    "    for topic, gen in zip(topics, generations):\n",
    "        inputs = nli_tokenizer(topic, gen, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = nli_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        entail_prob = probs[:, 0]  # label 0 = entailment\n",
    "        loss = 1.0 - entail_prob.mean()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def semantic_similarity_loss(refs, hypos):\n",
    "    losses = []\n",
    "    for r, h in zip(refs, hypos):\n",
    "        inputs = nli_tokenizer(r, h, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = nli_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        similarity_prob = probs[:, 0]  # entailment\n",
    "        losses.append(similarity_prob.mean())\n",
    "    return 1.0 - torch.stack(losses).mean()\n",
    "\n",
    "# === New: Argument generation detection using simple keyword pattern ===\n",
    "def argument_presence_loss(paragraphs):\n",
    "    keywords = [\"because\", \"as a result\", \"due to\", \"this means\", \"this is because\", \"for example\", \"for instance\"]\n",
    "    losses = []\n",
    "    for para in paragraphs:\n",
    "        score = any(k in para.lower() for k in keywords)\n",
    "        loss = 0.0 if score else 1.0\n",
    "        losses.append(torch.tensor(loss))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def lexical_diversity_loss(labels, pad_token_id=0):\n",
    "    losses = []\n",
    "    for seq in labels:\n",
    "        words = [t for t in seq if t != pad_token_id]\n",
    "        unique = len(set(words))\n",
    "        total = len(words)\n",
    "        penalty = 1.0 - unique / total if total > 0 else 0.0\n",
    "        losses.append(torch.tensor(penalty, device=labels.device))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def repetition_overlap_loss(body1s, body2s):\n",
    "    losses = []\n",
    "    for b1, b2 in zip(body1s, body2s):\n",
    "        set1 = set(b1.lower().split())\n",
    "        set2 = set(b2.lower().split())\n",
    "        overlap = len(set1 & set2) / max(1, len(set2))\n",
    "        losses.append(torch.tensor(overlap))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def ngram_overlap_loss(sequences, n=3):\n",
    "    losses = []\n",
    "    for seq in sequences:\n",
    "        tokens = seq.lower().split()\n",
    "        ngrams = set(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "        losses.append(torch.tensor(1.0 - len(ngrams) / max(1, len(tokens)), device='cpu'))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def argument_distance_loss(body1s, body2s):\n",
    "    return semantic_similarity_loss(body1s, body2s)  # Higher similarity → higher loss\n",
    "\n",
    "def dynamic_mask_input(text, tokenizer, mask_rate=0.15):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) < 4:\n",
    "        return text\n",
    "    num_to_mask = max(1, int(len(tokens) * mask_rate))\n",
    "    for i in sample(range(len(tokens)), num_to_mask):\n",
    "        tokens[i] = \"<extra_id_0>\"\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "def t5_paraphrase_text(text):\n",
    "    result = paraphraser(f\"paraphrase: {text} </s>\", max_length=128, num_return_sequences=1, do_sample=True)\n",
    "    return result[0][\"generated_text\"] if result else text\n",
    "\n",
    "# === Load and prepare dataset ===\n",
    "raw_dataset = load_dataset(\"chillies/IELTS-writing-task-2-evaluation\", split=\"train\")\n",
    "\n",
    "def is_valid(example):\n",
    "    try:\n",
    "        band = float(re.sub(r\"[^\\d.]\", \"\", example[\"band\"]))\n",
    "        return band >= 7.0 and example[\"essay\"] and len(example[\"essay\"].split()) > 220\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "filtered = [ex for ex in raw_dataset if is_valid(ex)]\n",
    "\n",
    "def split_paragraphs_flex(essay):\n",
    "    paras = [p.strip() for p in re.split(r\"\\n{2,}\", essay.strip()) if p.strip()]\n",
    "    return paras[0], paras[1], paras[2], paras[-1] if len(paras) >= 4 else None\n",
    "\n",
    "split_data = []\n",
    "for ex in filtered:\n",
    "    try:\n",
    "        result = split_paragraphs_flex(ex[\"essay\"])\n",
    "        if result is None:\n",
    "            continue\n",
    "        intro, body1, body2, conclusion = result\n",
    "        if all(len(p.split()) > t for p, t in zip([intro, body1, body2, conclusion], [40, 60, 70, 35])) and ex[\"prompt\"][:30] not in intro:\n",
    "            set1, set2 = set(body1.lower().split()), set(body2.lower().split())\n",
    "            if len(set1 & set2) / max(1, len(set2)) < 0.7:\n",
    "                split_data.append({\n",
    "                    \"prompt\": ex[\"prompt\"].strip(),\n",
    "                    \"intro\": intro.strip(),\n",
    "                    \"body1\": body1.strip(),\n",
    "                    \"body2\": body2.strip(),\n",
    "                    \"conclusion\": conclusion.strip()\n",
    "                })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(\"\\n📊 Filtered Samples:\", len(split_data))\n",
    "\n",
    "# === Define train function ===\n",
    "def train_paragraph_model(field, save_dir, max_target_length=256):\n",
    "    print(f\"\\n🚀 Training for: {field.upper()}\", flush=True)\n",
    "    data = []\n",
    "    for ex in split_data:\n",
    "        if len(ex[\"prompt\"]) < 10 or len(ex[field]) < 30:\n",
    "            continue\n",
    "        prompt = dynamic_mask_input(ex[\"prompt\"], tokenizer) if random() < 0.5 else ex[\"prompt\"]\n",
    "        if field == \"intro\":\n",
    "            input_text = f\"Write a short and clear INTRODUCTION:\\n\\n{prompt}\\n\\n- Paraphrase topic\\n- State opinion\\n- Brief background\"\n",
    "        elif field == \"body1\":\n",
    "            input_text = f\"Write the FIRST BODY PARAGRAPH for:\\n\\n{prompt}\\n\\n- Clear argument\\n- Specific example\\n- Logical explanation\"\n",
    "        elif field == \"body2\":\n",
    "            intro = dynamic_mask_input(ex[\"intro\"], tokenizer) if random() < 0.5 else ex[\"intro\"]\n",
    "            body1_masked = dynamic_mask_input(ex[\"body1\"], tokenizer) if random() < 0.3 else ex[\"body1\"]\n",
    "            topic_masked = dynamic_mask_input(prompt, tokenizer) if random() < 0.3 else prompt\n",
    "            input_text = (\n",
    "                f\"Write the SECOND BODY PARAGRAPH that presents a CONTRASTING perspective.\\n\\n\"\n",
    "                f\"TOPIC: {topic_masked}\\n\\nINTRO: {intro}\\n\\nBODY 1: {body1_masked}\\n\\n\"\n",
    "                \"Requirements:\\n- Start with a contrast linker\\n- Opposing idea\\n- Specific example\\n- Avoid repeating Body 1\"\n",
    "            )\n",
    "        elif field == \"conclusion\":\n",
    "            intro = t5_paraphrase_text(ex[\"intro\"]) if random() < 0.6 else ex[\"intro\"]\n",
    "            input_text = (\n",
    "                f\"Write a CONCLUSION:\\n\\nTOPIC: {prompt}\\n\\nINTRO (paraphrased): {intro}\\n\\n\"\n",
    "                \"Instructions:\\n- Restate opinion\\n- Summarise main points\\n- End strongly\"\n",
    "            )\n",
    "        data.append({\n",
    "            \"input_text\": input_text,\n",
    "            \"target_text\": ex[field],\n",
    "            \"intro\": ex[\"intro\"],\n",
    "            \"body1\": ex[\"body1\"],\n",
    "            \"body2\": ex[\"body2\"],\n",
    "            \"topic\": ex[\"prompt\"]\n",
    "        })\n",
    "\n",
    "    dataset = Dataset.from_list(data).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        if field in [\"body2\", \"conclusion\"]:\n",
    "            intros = tokenizer(batch[\"intro\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "            inputs[\"intro\"] = intros[\"input_ids\"]\n",
    "        if field in [\"body1\", \"body2\"]:\n",
    "            inputs[\"body1_text\"] = batch[\"body1\"]\n",
    "        if field == \"body2\":\n",
    "            inputs[\"body2_text\"] = batch[\"body2\"]\n",
    "            inputs[\"topic\"] = batch[\"topic\"]\n",
    "        return inputs\n",
    "\n",
    "    tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    lora = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    model = get_peft_model(model, lora)\n",
    "\n",
    "    # === Inside dpo_loss: add argument_presence_loss ===\n",
    "    # === Inside dpo_loss: add argument_presence_loss for body1 and body2 ===\n",
    "    def dpo_loss(logits, labels, intros=None, body1_text=None, body2_text=None, topic_text=None, pad_token_id=0):\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        mask = labels != pad_token_id\n",
    "        base = F.cross_entropy(logits[mask], labels[mask]) if mask.any() else torch.tensor(0.0, device=logits.device)\n",
    "        l_lex = lexical_diversity_loss(labels.view(1, -1), pad_token_id)\n",
    "        l_contra = nli_contradiction_loss(body1_text, body2_text) if body1_text is not None else 0.0\n",
    "        l_topic = topic_relevance_loss(topic_text, body2_text) * 0.9 if topic_text is not None else 0.0\n",
    "        l_rep = repetition_overlap_loss(body1_text, body2_text) if body1_text is not None else 0.0\n",
    "        l_ngram = ngram_overlap_loss(body2_text) if body2_text is not None else 0.0\n",
    "        l_arg = argument_distance_loss(body1_text, body2_text) if body1_text is not None else 0.0\n",
    "        l_sem = semantic_similarity_loss(intros, body2_text) if intros is not None and body2_text is not None else 0.0\n",
    "        l_gen_b2 = argument_presence_loss(body2_text) if body2_text is not None else 0.0\n",
    "        l_gen_b1 = argument_presence_loss(body1_text) if body1_text is not None else 0.0\n",
    "        return base + 0.2 * l_lex + 0.7 * l_contra + 0.9 * l_topic + 0.5 * l_rep + 0.4 * l_ngram + 0.4 * l_arg + 0.4 * l_sem + 0.5 * l_gen_b2 + 0.3 * l_gen_b1\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            intros = inputs.get(\"intro\")\n",
    "            body1_text = inputs.get(\"body1_text\")\n",
    "            body2_text = inputs.get(\"body2_text\")\n",
    "            topic_text = inputs.get(\"topic\")\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "            loss = dpo_loss(outputs.logits, labels, intros, body1_text, body2_text, topic_text)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        learning_rate=3e-4,\n",
    "        warmup_steps=100,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{save_dir}/logs\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=20,\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"test\"]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(f\"✅ Saved model to: {save_dir}\", flush=True)\n",
    "\n",
    "# === Train all paragraph models for v23 ===\n",
    "train_paragraph_model(\"intro\", \"./t5_intro_lora_v23\", max_target_length=160)\n",
    "train_paragraph_model(\"body1\", \"./t5_body1_lora_v23\", max_target_length=240)\n",
    "train_paragraph_model(\"body2\", \"./t5_body2_lora_v23\", max_target_length=288)\n",
    "train_paragraph_model(\"conclusion\", \"./t5_conclusion_lora_v23\", max_target_length=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSeq2SeqLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "C:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Temp\\ipykernel_29120\\4088496137.py:26: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  return HuggingFacePipeline(pipeline=pipe)\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSeq2SeqLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSeq2SeqLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForSeq2SeqLM' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "C:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Temp\\ipykernel_29120\\4088496137.py:89: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain_intro = LLMChain(llm=llm_intro, prompt=prompt_intro, output_key=\"intro\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 📝 IELTS Essay 1 ====================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Temp\\ipykernel_29120\\4088496137.py:148: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = full_essay_chain({\"topic\": topic})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 INTRO:\n",
      " Some people have claimed that the interview is not a reliable way of choosing the right person to work for. I agree that there are other methods, which can be used to help the candidate. In my opinion, I believe that the interview should be a more reliable method of selecting someone. \n",
      "\n",
      "🟩 BODY 1:\n",
      " To begin with, people are accustomed to interviewing and it is said that they do not have the knowledge of the business or their skills. They think that this is because they do not know how to handle the company effectively. Therefore, they do not get the chance to understand the business process in detail and make the decision on what to do with the employee. For instance, they are given the opportunity to study the business plan and will find out the benefits of doing so. \n",
      "\n",
      "🟨 BODY 2:\n",
      " On the other hand, there are many other methods to choose the right person. In addition, there are also other techniques, which can be used to help the candidate. For instance, they have a chance to know about the culture of the company and their culture. For instance; they get the opportunity to study the business plan and the benefits of doing so. Then, they will be given the chance to meet the management team and make the decision on how to hire the right person for the job. \n",
      "\n",
      "🟥 CONCLUSION:\n",
      " To conclude, although the interview can be a reliable method of selecting the right person, there are other methods which can be used to help the candidate. However, I believe that the interview is not a reliable way of choosing someone who is good in his field. \n",
      "\n",
      "📊 Evaluation:\n",
      "Word Count: 275\n",
      "Repeated Words (>2): ['that', 'have', 'help', 'interview', 'right', 'chance', 'reliable', 'business', 'other', 'there', 'used', 'which', 'candidate.', 'they']\n",
      "Grammar Errors: 2\n",
      "Grammar Issues (Sample): ['Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'Consider using “there are other” or “there are also”']\n",
      "Coherence Score: 1.0\n",
      "Intro-Conclusion Logic Conflict: aligned\n",
      "Has Example: True\n",
      "Body1-Body2 Overlap: 0.49\n",
      "Body1-Body2 Contradiction: No\n",
      "Body1 Has Argument: True\n",
      "Body2 Has Argument: True\n",
      "\n",
      "==================== 📝 IELTS Essay 2 ====================\n",
      "\n",
      "🔹 INTRO:\n",
      " In the recent years, many children find it difficult to concentrate on school or do their homework. This essay will discuss the reasons behind this and provide solutions for this issue. \n",
      "\n",
      "🟩 BODY 1:\n",
      " To begin with, children find it hard to concentrate on school. This is due to the fact that they can’t concentrate on their work and be distracted by them. In this way, they lack the time for studying. For instance, they don’t have enough time to study a book, or go to the gym. It is because of this reason that they cannot concentrate on their studies. So, there are many reasons why they struggle to focus on their studies in addition to their academics. \n",
      "\n",
      "🟨 BODY 2:\n",
      " In the second point, we can solve this problem by identifying the causes. For instance, children often have difficulty concentrating on school or work. This is because of the fact that they don’t have enough time to do their homework. They are not able to concentrate on their studies. Furthermore, they have to focus on their studies and be aware of other issues. For instance: they need to study a new book or a new movie. \n",
      "\n",
      "🟥 CONCLUSION:\n",
      " In conclusion, it is important to understand why children have difficulty in concentrating on school or homework. The IELTS Essay Task 2 will provide solutions for this problem. \n",
      "\n",
      "📊 Evaluation:\n",
      "Word Count: 220\n",
      "Repeated Words (>2): ['this', 'that', 'homework.', 'their', 'have', 'time', 'concentrate', 'children', 'school', 'they']\n",
      "Grammar Errors: 2\n",
      "Grammar Issues (Sample): ['The definite article “the” is not necessary in this context.', 'This phrase is redundant. Consider using “because”.']\n",
      "Coherence Score: 1.0\n",
      "Intro-Conclusion Logic Conflict: aligned\n",
      "Has Example: True\n",
      "Body1-Body2 Overlap: 0.54\n",
      "Body1-Body2 Contradiction: No\n",
      "Body1 Has Argument: True\n",
      "Body2 Has Argument: True\n",
      "\n",
      "==================== 📝 IELTS Essay 3 ====================\n",
      "\n",
      "🔹 INTRO:\n",
      " Climate change is a serious problem and many people argue that it should be solved by reducing the emissions from the atmosphere. This essay will examine the topic and provide some reasons for this statement. \n",
      "\n",
      "🟩 BODY 1:\n",
      " Firstly, climate change is the main reason why people are putting efforts to prevent it. For instance, we should start living in an environment where we can be more resilient. Therefore, it is important that we understand how to cope with it. However, this means that we can only deal with the consequences of climate change without having to live in an environment that is not sustainable. As a result, we can’t be able to escape from our environment without wasting time and money. \n",
      "\n",
      "🟨 BODY 2:\n",
      " In addition, there are many ways that people can get rid of the impacts of climate change. For example, reducing the amount of carbon dioxide from our atmosphere will help us avoid the effects of warming. For example we can’t be able to take care of the environment without wasting money or energy. Besides, we can also reduce the amount of energy that we consume. This is why we should focus on developing new technologies such as solar cells and solar cells. \n",
      "\n",
      "🟥 CONCLUSION:\n",
      " In conclusion, IELTS Writing Task 2 will examine the issue of climate change. Although it is a serious problem, it can be solved by reducing the emissions from the atmosphere. To conclude, I think that in my opinion, people need to work with the problem. \n",
      "\n",
      "📊 Evaluation:\n",
      "Word Count: 246\n",
      "Repeated Words (>2): ['should', 'this', 'that', 'environment', 'reducing', 'climate', 'people', 'without', 'from', 'change', 'with', 'will']\n",
      "Grammar Errors: 1\n",
      "Grammar Issues (Sample): ['A comma is probably missing here.']\n",
      "Coherence Score: 1.0\n",
      "Intro-Conclusion Logic Conflict: aligned\n",
      "Has Example: True\n",
      "Body1-Body2 Overlap: 0.38\n",
      "Body1-Body2 Contradiction: No\n",
      "Body1 Has Argument: True\n",
      "Body2 Has Argument: True\n",
      "\n",
      "==================== 📝 IELTS Essay 4 ====================\n",
      "\n",
      "🔹 INTRO:\n",
      " It is no secret that we are confronted with an increasing number of advertisements from competing companies. We can see that consumers are influenced by the advertisements and it is important to know whether they are influenced or not. This essay will explain in detail how this affects our daily life. \n",
      "\n",
      "🟩 BODY 1:\n",
      " Firstly, the majority of the consumers are influenced by advertisements from other companies. In fact, they are not influenced by any particular product or service. However, it is important to point out that this is a sign of a high level of professionalism in the industry. For example, the latest ads from the major companies are available in the market and can be found in online stores. This is due to the fact that they are more likely to spend their money on advertising. Therefore, they tend to have a higher degree of awareness about their products and services. \n",
      "\n",
      "🟨 BODY 2:\n",
      " On the other hand, we should make a conscious decision about how to protect our daily lives. For instance, we can use the current trend of advertising from various companies in order to get the best deals and offers. This is because they are aware that they are not influenced by any particular product or service. Instead, they should be aware of the latest products and services from other companies. \n",
      "\n",
      "🟥 CONCLUSION:\n",
      " In conclusion, we should understand how consumers are influenced by advertisements and what measures can be taken to protect them. There are many factors that affect our everyday life and it is very important to understand how they are influenced or not. \n",
      "\n",
      "📊 Evaluation:\n",
      "Word Count: 262\n",
      "Repeated Words (>2): ['this', 'should', 'that', 'consumers', 'companies.', 'they', 'other', 'influenced', 'from', 'important', 'advertisements']\n",
      "Grammar Errors: 2\n",
      "Grammar Issues (Sample): ['Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).']\n",
      "Coherence Score: 1.0\n",
      "Intro-Conclusion Logic Conflict: aligned\n",
      "Has Example: True\n",
      "Body1-Body2 Overlap: 0.54\n",
      "Body1-Body2 Contradiction: No\n",
      "Body1 Has Argument: True\n",
      "Body2 Has Argument: True\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2 (v23 - Stronger Eval + Argument Check BODY1/BODY2) ===\n",
    "import re\n",
    "import language_tool_python\n",
    "from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n",
    "from peft import PeftModel\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# === Load fine-tuned models ===\n",
    "def load_lora_hf_pipeline(path, base=\"t5-base\", max_length=320):\n",
    "    base_model = T5ForConditionalGeneration.from_pretrained(base)\n",
    "    model = PeftModel.from_pretrained(base_model, path)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=4\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "llm_intro = load_lora_hf_pipeline(\"./t5_intro_lora_v23\", max_length=160)\n",
    "llm_body1 = load_lora_hf_pipeline(\"./t5_body1_lora_v23\", max_length=240)\n",
    "llm_body2 = load_lora_hf_pipeline(\"./t5_body2_lora_v23\", max_length=288)\n",
    "llm_concl = load_lora_hf_pipeline(\"./t5_conclusion_lora_v23\", max_length=96)\n",
    "\n",
    "# === Prompt templates ===\n",
    "prompt_intro = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=(\n",
    "        \"Write a short and clear INTRODUCTION for this IELTS Writing Task 2 topic:\\n\\n{topic}\\n\\n\"\n",
    "        \"Requirements:\\n\"\n",
    "        \"- Paraphrase the topic clearly\\n\"\n",
    "        \"- State your opinion\\n\"\n",
    "        \"- Add brief context\\n\"\n",
    "        \"- Stay strictly on topic. Do NOT introduce unrelated content\\n\"\n",
    "        \"- Use formal academic tone\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_body1 = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=(\n",
    "        \"Write the FIRST BODY PARAGRAPH for this IELTS Writing Task 2 topic:\\n\\n{topic}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Present ONE clear reason to support your opinion\\n\"\n",
    "        \"- Provide logic and ONE specific example\\n\"\n",
    "        \"- Avoid vague generalisations and unrelated facts\\n\"\n",
    "        \"- Keep ideas fully aligned with the topic above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_body2 = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"intro\", \"body1\"],\n",
    "    template=(\n",
    "        \"Write the SECOND BODY PARAGRAPH for this IELTS Writing Task 2 essay.\\n\\n\"\n",
    "        \"TOPIC: {topic}\\n\\n\"\n",
    "        \"INTRO: {intro}\\n\\n\"\n",
    "        \"BODY 1: {body1}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Present a CLEAR CONTRASTING viewpoint\\n\"\n",
    "        \"- Use a new idea and a different example\\n\"\n",
    "        \"- DO NOT repeat phrases, ideas, or examples from Body 1\\n\"\n",
    "        \"- Begin with a contrast linker (e.g., 'However', 'On the other hand')\\n\"\n",
    "        \"- Stay strictly relevant to the given TOPIC\"\n",
    "    )\n",
    ")\n",
    "\n",
    "prompt_concl = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"intro\"],\n",
    "    template=(\n",
    "        \"Write a CONCLUSION paragraph for this IELTS Writing Task 2 essay.\\n\\n\"\n",
    "        \"TOPIC: {topic}\\n\\n\"\n",
    "        \"INTRODUCTION: {intro}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"- Restate your opinion using DIFFERENT words\\n\"\n",
    "        \"- Briefly summarise both body paragraphs\\n\"\n",
    "        \"- End with a strong final sentence (recommendation or reflection)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# === Chains ===\n",
    "chain_intro = LLMChain(llm=llm_intro, prompt=prompt_intro, output_key=\"intro\")\n",
    "chain_body1 = LLMChain(llm=llm_body1, prompt=prompt_body1, output_key=\"body1\")\n",
    "chain_body2 = LLMChain(llm=llm_body2, prompt=prompt_body2, output_key=\"body2\")\n",
    "chain_concl = LLMChain(llm=llm_concl, prompt=prompt_concl, output_key=\"conclusion\")\n",
    "\n",
    "full_essay_chain = SequentialChain(\n",
    "    chains=[chain_intro, chain_body1, chain_body2, chain_concl],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"intro\", \"body1\", \"body2\", \"conclusion\"],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# === Evaluation Function ===\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "def evaluate_essay(paragraphs):\n",
    "    full_text = \" \".join(paragraphs)\n",
    "    word_count = len(full_text.split())\n",
    "    repeated = [w for w in set(full_text.lower().split()) if full_text.lower().split().count(w) > 2 and len(w) > 3]\n",
    "    grammar_matches = tool.check(full_text)\n",
    "    grammar_errors = len(grammar_matches)\n",
    "    grammar_samples = [match.message for match in grammar_matches[:3]]\n",
    "    coherence_words = [\"first\", \"second\", \"furthermore\", \"however\", \"in conclusion\", \"to begin\", \"on the other hand\"]\n",
    "    coherence_score = 1.0 if any(w in full_text.lower() for w in coherence_words) else 0.5\n",
    "    intro = paragraphs[0].lower()\n",
    "    concl = paragraphs[-1].lower()\n",
    "    conflict = \"contradict\" if ((\"agree\" in intro and \"disagree\" in concl) or (\"disagree\" in intro and \"agree\" in concl)) else \"aligned\"\n",
    "    has_example = any(kw in full_text.lower() for kw in [\"for example\", \"such as\", \"for instance\"])\n",
    "\n",
    "    body1_set = set(paragraphs[1].lower().split())\n",
    "    body2_set = set(paragraphs[2].lower().split())\n",
    "    overlap = len(body1_set & body2_set) / max(1, len(body2_set))\n",
    "    contradiction = \"Yes\" if overlap < 0.3 and any(w in paragraphs[2].lower() for w in [\"however\", \"on the other hand\", \"in contrast\"]) else \"No\"\n",
    "\n",
    "    # === Argument keyword check for BODY1 and BODY2 ===\n",
    "    arg_keywords = [\"because\", \"as a result\", \"due to\", \"for example\", \"for instance\", \"this is because\", \"consequently\"]\n",
    "    def has_argument_keywords(text):\n",
    "        return any(k in text.lower() for k in arg_keywords)\n",
    "\n",
    "    body1_argument = has_argument_keywords(paragraphs[1])\n",
    "    body2_argument = has_argument_keywords(paragraphs[2])\n",
    "\n",
    "    return {\n",
    "        \"Word Count\": word_count,\n",
    "        \"Repeated Words (>2)\": repeated,\n",
    "        \"Grammar Errors\": grammar_errors,\n",
    "        \"Grammar Issues (Sample)\": grammar_samples,\n",
    "        \"Coherence Score\": coherence_score,\n",
    "        \"Intro-Conclusion Logic Conflict\": conflict,\n",
    "        \"Has Example\": has_example,\n",
    "        \"Body1-Body2 Overlap\": round(overlap, 2),\n",
    "        \"Body1-Body2 Contradiction\": contradiction,\n",
    "        \"Body1 Has Argument\": body1_argument,\n",
    "        \"Body2 Has Argument\": body2_argument\n",
    "    }\n",
    "\n",
    "# === Generate and evaluate essays ===\n",
    "def generate_until_min_words(topic, min_words=220, max_tries=5):\n",
    "    for _ in range(max_tries):\n",
    "        result = full_essay_chain({\"topic\": topic})\n",
    "        paragraphs = [result[k] for k in [\"intro\", \"body1\", \"body2\", \"conclusion\"]]\n",
    "        evaluation = evaluate_essay(paragraphs)\n",
    "        if evaluation[\"Word Count\"] >= min_words and evaluation[\"Body1-Body2 Overlap\"] <= 0.6:\n",
    "            return paragraphs, evaluation\n",
    "    return paragraphs, evaluation\n",
    "\n",
    "# === Topics ===\n",
    "topics = [\n",
    "    \"Interviews form the basic criteria for most large companies. However, some people think that the interview is not a reliable method of choosing whom to employ and there are other better methods. To what extent do you agree or disagree?\",\n",
    "    \"Children find it difficult to concentrate on or pay attention to school. What are the reasons? How can we solve this problem?\",\n",
    "    \"Some people think that instead of preventing climate change, we need to find a way to live with it. To what extent do you agree or disagree?\",\n",
    "    \"Consumers are faced with increasing numbers of advertisements from competing companies. To what extent do you think are consumers influenced by advertisement? What measures can be taken to protect them?\"\n",
    "]\n",
    "\n",
    "# === Run and print ===\n",
    "for i, topic in enumerate(topics, 1):\n",
    "    print(f\"\\n{'='*20} 📝 IELTS Essay {i} {'='*20}\\n\")\n",
    "    paragraphs, evaluation = generate_until_min_words(topic)\n",
    "    print(\"🔹 INTRO:\\n\", paragraphs[0], \"\\n\")\n",
    "    print(\"🟩 BODY 1:\\n\", paragraphs[1], \"\\n\")\n",
    "    print(\"🟨 BODY 2:\\n\", paragraphs[2], \"\\n\")\n",
    "    print(\"🟥 CONCLUSION:\\n\", paragraphs[3], \"\\n\")\n",
    "    print(\"📊 Evaluation:\")\n",
    "    for k, v in evaluation.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
