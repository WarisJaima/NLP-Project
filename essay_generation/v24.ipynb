{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Filtered Samples: 419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: st124689 (binit-ait) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\VUONGLOCTRUONG\\Downloads\\quiz2\\wandb\\run-20250410_151652-k6ogzj6m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/nlp_project2/runs/k6ogzj6m' target=\"_blank\">intro_v24</a></strong> to <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/nlp_project2/runs/k6ogzj6m' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/k6ogzj6m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training for: INTRO\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec467e6953514ae49a18e1a484b72ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d402ec3da364d158f90e243a945be9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 06:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Semantic Score</th>\n",
       "      <th>Diversity Score</th>\n",
       "      <th>Avg F1</th>\n",
       "      <th>Final Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.259900</td>\n",
       "      <td>2.719442</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.659200</td>\n",
       "      <td>2.641587</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.491700</td>\n",
       "      <td>0.751200</td>\n",
       "      <td>0.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.783600</td>\n",
       "      <td>2.606685</td>\n",
       "      <td>0.239700</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>0.757400</td>\n",
       "      <td>0.914100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.736600</td>\n",
       "      <td>2.594213</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.456800</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.922700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "✅ Saved model to: ./t5_intro_lora_v24\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/avg_f1</td><td>▁▄█▆</td></tr><tr><td>eval/diversity_score</td><td>█▆▂▁</td></tr><tr><td>eval/final_score</td><td>▁▃▇█</td></tr><tr><td>eval/loss</td><td>█▄▂▁</td></tr><tr><td>eval/runtime</td><td>▃▁█▆</td></tr><tr><td>eval/samples_per_second</td><td>▅█▁▃</td></tr><tr><td>eval/semantic_score</td><td>▇█▁▂</td></tr><tr><td>eval/steps_per_second</td><td>▅█▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/grad_norm</td><td>█▁▁▁▁▂▃▂▁▂▁▁▂▂▂▂▂▂▁▁▂▂▂▂▃▃▂▂▂▂▂▂▃▂▃▂▂▂▂▂</td></tr><tr><td>train/learning_rate</td><td>▁▃▄████▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>██▆▅▅▅▄▅▅▄▄▄▄▄▄▃▄▃▂▃▄▂▂▃▃▄▄▃▂▄▃▃▃▃▃▃▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/avg_f1</td><td>0.7547</td></tr><tr><td>eval/diversity_score</td><td>0.4568</td></tr><tr><td>eval/final_score</td><td>0.9227</td></tr><tr><td>eval/loss</td><td>2.59421</td></tr><tr><td>eval/runtime</td><td>23.4873</td></tr><tr><td>eval/samples_per_second</td><td>1.788</td></tr><tr><td>eval/semantic_score</td><td>0.241</td></tr><tr><td>eval/steps_per_second</td><td>0.894</td></tr><tr><td>total_flos</td><td>934702926004224.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>756</td></tr><tr><td>train/grad_norm</td><td>1.05182</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7366</td></tr><tr><td>train_loss</td><td>2.88314</td></tr><tr><td>train_runtime</td><td>412.7636</td></tr><tr><td>train_samples_per_second</td><td>3.653</td></tr><tr><td>train_steps_per_second</td><td>1.832</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">intro_v24</strong> at: <a href='https://wandb.ai/binit-ait/nlp_project2/runs/k6ogzj6m' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/k6ogzj6m</a><br> View project at: <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250410_151652-k6ogzj6m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\VUONGLOCTRUONG\\Downloads\\quiz2\\wandb\\run-20250410_152350-3kgo7sxk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/nlp_project2/runs/3kgo7sxk' target=\"_blank\">body1_v24</a></strong> to <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/nlp_project2/runs/3kgo7sxk' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/3kgo7sxk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training for: BODY1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7290b8f26b74a25bd08d6b0cbd7a606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418de0b9a41b48da8ce42d24a6314346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 29:58, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Semantic Score</th>\n",
       "      <th>Diversity Score</th>\n",
       "      <th>Avg F1</th>\n",
       "      <th>Final Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.460300</td>\n",
       "      <td>3.342158</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.730800</td>\n",
       "      <td>0.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.278800</td>\n",
       "      <td>3.281767</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.725900</td>\n",
       "      <td>0.736600</td>\n",
       "      <td>0.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.410300</td>\n",
       "      <td>3.261563</td>\n",
       "      <td>0.219200</td>\n",
       "      <td>0.721300</td>\n",
       "      <td>0.736200</td>\n",
       "      <td>0.669100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.352900</td>\n",
       "      <td>3.253983</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.724400</td>\n",
       "      <td>0.739800</td>\n",
       "      <td>0.667300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "✅ Saved model to: ./t5_body1_lora_v24\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/avg_f1</td><td>▁▆▅█</td></tr><tr><td>eval/diversity_score</td><td>█▆▁▄</td></tr><tr><td>eval/final_score</td><td>▅▁█▄</td></tr><tr><td>eval/loss</td><td>█▃▂▁</td></tr><tr><td>eval/runtime</td><td>█▁▂▄</td></tr><tr><td>eval/samples_per_second</td><td>▁█▆▃</td></tr><tr><td>eval/semantic_score</td><td>▁▆█▆</td></tr><tr><td>eval/steps_per_second</td><td>▁█▆▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>▁▂▅▅█▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▂▁▂▆▁</td></tr><tr><td>train/learning_rate</td><td>▂▄▄▆▆███▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▅▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/avg_f1</td><td>0.7398</td></tr><tr><td>eval/diversity_score</td><td>0.7244</td></tr><tr><td>eval/final_score</td><td>0.6673</td></tr><tr><td>eval/loss</td><td>3.25398</td></tr><tr><td>eval/runtime</td><td>25.6383</td></tr><tr><td>eval/samples_per_second</td><td>1.638</td></tr><tr><td>eval/semantic_score</td><td>0.2166</td></tr><tr><td>eval/steps_per_second</td><td>0.819</td></tr><tr><td>total_flos</td><td>934702926004224.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>756</td></tr><tr><td>train/grad_norm</td><td>0.99461</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>3.3529</td></tr><tr><td>train_loss</td><td>3.63535</td></tr><tr><td>train_runtime</td><td>1799.5447</td></tr><tr><td>train_samples_per_second</td><td>0.838</td></tr><tr><td>train_steps_per_second</td><td>0.42</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">body1_v24</strong> at: <a href='https://wandb.ai/binit-ait/nlp_project2/runs/3kgo7sxk' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/3kgo7sxk</a><br> View project at: <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250410_152350-3kgo7sxk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\VUONGLOCTRUONG\\Downloads\\quiz2\\wandb\\run-20250410_155406-55bfopmo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/nlp_project2/runs/55bfopmo' target=\"_blank\">body2_v24</a></strong> to <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/nlp_project2/runs/55bfopmo' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/55bfopmo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training for: BODY2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49296ccf3298492d9c695b44c2253013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4495a54d67d84ce8aac531c9889aa75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 35:28, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Semantic Score</th>\n",
       "      <th>Diversity Score</th>\n",
       "      <th>Avg F1</th>\n",
       "      <th>Final Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.406200</td>\n",
       "      <td>3.230798</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.769800</td>\n",
       "      <td>0.726300</td>\n",
       "      <td>0.606200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.285600</td>\n",
       "      <td>3.181254</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.727600</td>\n",
       "      <td>0.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.282400</td>\n",
       "      <td>3.161742</td>\n",
       "      <td>0.244400</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>0.727200</td>\n",
       "      <td>0.654600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.395700</td>\n",
       "      <td>3.150816</td>\n",
       "      <td>0.244900</td>\n",
       "      <td>0.744100</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "✅ Saved model to: ./t5_body2_lora_v24\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/avg_f1</td><td>▁▅▄█</td></tr><tr><td>eval/diversity_score</td><td>█▂▁▄</td></tr><tr><td>eval/final_score</td><td>▁██▅</td></tr><tr><td>eval/loss</td><td>█▄▂▁</td></tr><tr><td>eval/runtime</td><td>█▁▅▄</td></tr><tr><td>eval/samples_per_second</td><td>▁█▃▄</td></tr><tr><td>eval/semantic_score</td><td>█▁▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▁█▃▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂▃▃█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▃▄▄▅▇███▇▇▇▇▇▇▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▆█▇▇▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/avg_f1</td><td>0.7284</td></tr><tr><td>eval/diversity_score</td><td>0.7441</td></tr><tr><td>eval/final_score</td><td>0.6335</td></tr><tr><td>eval/loss</td><td>3.15082</td></tr><tr><td>eval/runtime</td><td>29.7689</td></tr><tr><td>eval/samples_per_second</td><td>1.411</td></tr><tr><td>eval/semantic_score</td><td>0.2449</td></tr><tr><td>eval/steps_per_second</td><td>0.705</td></tr><tr><td>total_flos</td><td>934702926004224.0</td></tr><tr><td>train/epoch</td><td>4</td></tr><tr><td>train/global_step</td><td>756</td></tr><tr><td>train/grad_norm</td><td>0.95556</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.3957</td></tr><tr><td>train_loss</td><td>3.48264</td></tr><tr><td>train_runtime</td><td>2129.4328</td></tr><tr><td>train_samples_per_second</td><td>0.708</td></tr><tr><td>train_steps_per_second</td><td>0.355</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">body2_v24</strong> at: <a href='https://wandb.ai/binit-ait/nlp_project2/runs/55bfopmo' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/55bfopmo</a><br> View project at: <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250410_155406-55bfopmo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\VUONGLOCTRUONG\\Downloads\\quiz2\\wandb\\run-20250410_162954-zbuioiug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/nlp_project2/runs/zbuioiug' target=\"_blank\">conclusion_v24</a></strong> to <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/nlp_project2' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/nlp_project2/runs/zbuioiug' target=\"_blank\">https://wandb.ai/binit-ait/nlp_project2/runs/zbuioiug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training for: CONCLUSION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f9f084a41f4516a5976a0e88061990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d914fbaa85f04f3ca2f4c9843e52bda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='756' max='756' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [756/756 04:15, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Semantic Score</th>\n",
       "      <th>Diversity Score</th>\n",
       "      <th>Avg F1</th>\n",
       "      <th>Final Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.190500</td>\n",
       "      <td>2.887333</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.735100</td>\n",
       "      <td>0.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.201800</td>\n",
       "      <td>2.841069</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.740400</td>\n",
       "      <td>0.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.913800</td>\n",
       "      <td>2.824035</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.740300</td>\n",
       "      <td>0.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.009000</td>\n",
       "      <td>2.816853</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.463200</td>\n",
       "      <td>0.741800</td>\n",
       "      <td>0.935300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "🧪 Starting evaluation...\n",
      "✅ Saved model to: ./t5_conclusion_lora_v24\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1 (v24 - + Argument Generation Loss) ===\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, pipeline\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from random import sample, random\n",
    "from transformers import AutoTokenizer as AutoTokenizerNLI, AutoModelForSequenceClassification\n",
    "import wandb\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load tokenizer and paraphraser globally\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"ramsrigouthamg/t5_paraphraser\")\n",
    "\n",
    "# Load NLI model\n",
    "nli_tokenizer = AutoTokenizerNLI.from_pretrained(\"facebook/bart-large-mnli\")\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\").eval()\n",
    "\n",
    "# Load SBERT for semantic embedding\n",
    "sbert_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def distinct_ngrams(texts, n=2):\n",
    "    all_ngrams = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "        all_ngrams.extend(ngrams)\n",
    "    unique_ngrams = len(set(all_ngrams))\n",
    "    total_ngrams = len(all_ngrams)\n",
    "    return unique_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
    "\n",
    "def compute_metrics(eval_pred, lam):\n",
    "    print(\"🧪 Starting evaluation...\")\n",
    "    predictions = torch.argmax(torch.tensor(eval_pred.predictions[0]), dim=-1)\n",
    "    labels = torch.tensor(eval_pred.label_ids)\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    predicted_texts = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels_filtered = [[token if token != -100 else tokenizer.pad_token_id for token in seq] for seq in labels.cpu().numpy()]\n",
    "    true_texts = tokenizer.batch_decode(labels_filtered, skip_special_tokens=True)\n",
    "\n",
    "    # Semantic similarity\n",
    "    pred_emb = sbert_model.encode(predicted_texts, convert_to_tensor=True)\n",
    "    true_emb = sbert_model.encode(true_texts, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(pred_emb, true_emb)\n",
    "    semantic_score = cosine_scores.mean().item()\n",
    "\n",
    "    # Diversity score (distinct-2)\n",
    "    diversity_score = distinct_ngrams(predicted_texts, n=2)\n",
    "\n",
    "    # Word-level F1\n",
    "    all_f1 = []\n",
    "    for pred, label in zip(predicted_texts, true_texts):\n",
    "        pred_tokens = pred.split()\n",
    "        label_tokens = label.split()\n",
    "        y_true = [1 if t in label_tokens else 0 for t in pred_tokens]\n",
    "        y_pred = [1]*len(pred_tokens)\n",
    "        if len(y_true) > 0:\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "            all_f1.append(f1)\n",
    "    avg_f1 = sum(all_f1) / len(all_f1) if all_f1 else 0.0\n",
    "\n",
    "    final_score = (semantic_score * lam) + (diversity_score * 1-lam)\n",
    "\n",
    "    return {\n",
    "        \"semantic_score\": round(semantic_score, 4),\n",
    "        \"diversity_score\": round(diversity_score, 4),\n",
    "        \"avg_f1\": round(avg_f1, 4),\n",
    "        \"final_score\": round(1 - final_score, 4)\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def nli_contradiction_loss(premises, hypotheses):\n",
    "    losses = []\n",
    "    for premise, hypo in zip(premises, hypotheses):\n",
    "        inputs = nli_tokenizer(premise, hypo, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = nli_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        contradiction_prob = probs[:, 2]  # label 2 = contradiction\n",
    "        loss = 1.0 - contradiction_prob.mean()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def topic_relevance_loss(topics, generations):\n",
    "    losses = []\n",
    "    for topic, gen in zip(topics, generations):\n",
    "        inputs = nli_tokenizer(topic, gen, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = nli_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        entail_prob = probs[:, 0]  # label 0 = entailment\n",
    "        loss = 1.0 - entail_prob.mean()\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def semantic_similarity_loss(refs, hypos):\n",
    "    losses = []\n",
    "    for r, h in zip(refs, hypos):\n",
    "        inputs = nli_tokenizer(r, h, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = nli_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        similarity_prob = probs[:, 0]  # entailment\n",
    "        losses.append(similarity_prob.mean())\n",
    "    return 1.0 - torch.stack(losses).mean()\n",
    "\n",
    "# === New: Argument generation detection using simple keyword pattern ===\n",
    "def argument_presence_loss(paragraphs):\n",
    "    keywords = [\"because\", \"as a result\", \"due to\", \"this means\", \"this is because\", \"for example\", \"for instance\"]\n",
    "    losses = []\n",
    "    for para in paragraphs:\n",
    "        score = any(k in para.lower() for k in keywords)\n",
    "        loss = 0.0 if score else 1.0\n",
    "        losses.append(torch.tensor(loss))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def lexical_diversity_loss(labels, pad_token_id=0):\n",
    "    losses = []\n",
    "    for seq in labels:\n",
    "        words = [t for t in seq if t != pad_token_id]\n",
    "        unique = len(set(words))\n",
    "        total = len(words)\n",
    "        penalty = 1.0 - unique / total if total > 0 else 0.0\n",
    "        losses.append(torch.tensor(penalty, device=labels.device))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def repetition_overlap_loss(body1s, body2s):\n",
    "    losses = []\n",
    "    for b1, b2 in zip(body1s, body2s):\n",
    "        set1 = set(b1.lower().split())\n",
    "        set2 = set(b2.lower().split())\n",
    "        overlap = len(set1 & set2) / max(1, len(set2))\n",
    "        losses.append(torch.tensor(overlap))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def ngram_overlap_loss(sequences, n=3):\n",
    "    losses = []\n",
    "    for seq in sequences:\n",
    "        tokens = seq.lower().split()\n",
    "        ngrams = set(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))\n",
    "        losses.append(torch.tensor(1.0 - len(ngrams) / max(1, len(tokens)), device='cpu'))\n",
    "    return torch.stack(losses).mean()\n",
    "\n",
    "def argument_distance_loss(body1s, body2s):\n",
    "    return semantic_similarity_loss(body1s, body2s)  # Higher similarity → higher loss\n",
    "\n",
    "def dynamic_mask_input(text, tokenizer, mask_rate=0.15):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) < 4:\n",
    "        return text\n",
    "    num_to_mask = max(1, int(len(tokens) * mask_rate))\n",
    "    for i in sample(range(len(tokens)), num_to_mask):\n",
    "        tokens[i] = \"<extra_id_0>\"\n",
    "    return tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "def t5_paraphrase_text(text):\n",
    "    result = paraphraser(f\"paraphrase: {text} </s>\", max_length=128, num_return_sequences=1, do_sample=True)\n",
    "    return result[0][\"generated_text\"] if result else text\n",
    "\n",
    "# === Load and prepare dataset ===\n",
    "raw_dataset = load_dataset(\"chillies/IELTS-writing-task-2-evaluation\", split=\"train\")\n",
    "\n",
    "def is_valid(example):\n",
    "    try:\n",
    "        band = float(re.sub(r\"[^\\d.]\", \"\", example[\"band\"]))\n",
    "        return band >= 7.0 and example[\"essay\"] and len(example[\"essay\"].split()) > 220\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "filtered = [ex for ex in raw_dataset if is_valid(ex)]\n",
    "\n",
    "def split_paragraphs_flex(essay):\n",
    "    paras = [p.strip() for p in re.split(r\"\\n{2,}\", essay.strip()) if p.strip()]\n",
    "    return paras[0], paras[1], paras[2], paras[-1] if len(paras) >= 4 else None\n",
    "\n",
    "split_data = []\n",
    "for ex in filtered:\n",
    "    try:\n",
    "        result = split_paragraphs_flex(ex[\"essay\"])\n",
    "        if result is None:\n",
    "            continue\n",
    "        intro, body1, body2, conclusion = result\n",
    "        if all(len(p.split()) > t for p, t in zip([intro, body1, body2, conclusion], [40, 60, 70, 35])) and ex[\"prompt\"][:30] not in intro:\n",
    "            set1, set2 = set(body1.lower().split()), set(body2.lower().split())\n",
    "            if len(set1 & set2) / max(1, len(set2)) < 0.7:\n",
    "                split_data.append({\n",
    "                    \"prompt\": ex[\"prompt\"].strip(),\n",
    "                    \"intro\": intro.strip(),\n",
    "                    \"body1\": body1.strip(),\n",
    "                    \"body2\": body2.strip(),\n",
    "                    \"conclusion\": conclusion.strip()\n",
    "                })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(\"\\n📊 Filtered Samples:\", len(split_data))\n",
    "\n",
    "# === Define train function ===\n",
    "def train_paragraph_model(field, save_dir, max_target_length=256):\n",
    "    wandb.init(project=\"nlp_project2\", name=f\"{field}_v24\")\n",
    "\n",
    "    print(f\"\\n🚀 Training for: {field.upper()}\", flush=True)\n",
    "    data = []\n",
    "    for ex in split_data:\n",
    "        if len(ex[\"prompt\"]) < 10 or len(ex[field]) < 30:\n",
    "            continue\n",
    "        prompt = dynamic_mask_input(ex[\"prompt\"], tokenizer) if random() < 0.5 else ex[\"prompt\"]\n",
    "        if field == \"intro\":\n",
    "            input_text = f\"Write a short and clear INTRODUCTION:\\n\\n{prompt}\\n\\n- Paraphrase topic\\n- State opinion\\n- Brief background\"\n",
    "        elif field == \"body1\":\n",
    "            input_text = f\"Write the FIRST BODY PARAGRAPH for:\\n\\n{prompt}\\n\\n- Clear argument\\n- Specific example\\n- Logical explanation\"\n",
    "        elif field == \"body2\":\n",
    "            intro = dynamic_mask_input(ex[\"intro\"], tokenizer) if random() < 0.5 else ex[\"intro\"]\n",
    "            body1_masked = dynamic_mask_input(ex[\"body1\"], tokenizer) if random() < 0.3 else ex[\"body1\"]\n",
    "            topic_masked = dynamic_mask_input(prompt, tokenizer) if random() < 0.3 else prompt\n",
    "            input_text = (\n",
    "                f\"Write the SECOND BODY PARAGRAPH that presents a CONTRASTING perspective.\\n\\n\"\n",
    "                f\"TOPIC: {topic_masked}\\n\\nINTRO: {intro}\\n\\nBODY 1: {body1_masked}\\n\\n\"\n",
    "                \"Requirements:\\n- Start with a contrast linker\\n- Opposing idea\\n- Specific example\\n- Avoid repeating Body 1\"\n",
    "            )\n",
    "        elif field == \"conclusion\":\n",
    "            intro = t5_paraphrase_text(ex[\"intro\"]) if random() < 0.5 else ex[\"intro\"]\n",
    "            input_text = (\n",
    "                f\"Write a CONCLUSION:\\n\\nTOPIC: {prompt}\\n\\nINTRO (paraphrased): {intro}\\n\\n\"\n",
    "                \"Instructions:\\n- Restate opinion\\n- Summarise main points\\n- End strongly\"\n",
    "            )\n",
    "        data.append({\n",
    "            \"input_text\": input_text,\n",
    "            \"target_text\": ex[field],\n",
    "            \"intro\": ex[\"intro\"],\n",
    "            \"body1\": ex[\"body1\"],\n",
    "            \"body2\": ex[\"body2\"],\n",
    "            \"topic\": ex[\"prompt\"]\n",
    "        })\n",
    "\n",
    "    dataset = Dataset.from_list(data).train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "        targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=max_target_length)\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        if field in [\"body2\", \"conclusion\"]:\n",
    "            intros = tokenizer(batch[\"intro\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "            inputs[\"intro\"] = intros[\"input_ids\"]\n",
    "        if field in [\"body1\", \"body2\"]:\n",
    "            inputs[\"body1_text\"] = batch[\"body1\"]\n",
    "        if field == \"body2\":\n",
    "            inputs[\"body2_text\"] = batch[\"body2\"]\n",
    "            inputs[\"topic\"] = batch[\"topic\"]\n",
    "        return inputs\n",
    "\n",
    "    tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "    lora = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_2_SEQ_LM)\n",
    "    model = get_peft_model(model, lora)\n",
    "\n",
    "    # === Inside dpo_loss: add argument_presence_loss ===\n",
    "    # === Inside dpo_loss: add argument_presence_loss for body1 and body2 ===\n",
    "    def dpo_loss(logits, labels, intros=None, body1_text=None, body2_text=None, topic_text=None, pad_token_id=0):\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        labels = labels.view(-1)\n",
    "        mask = labels != pad_token_id\n",
    "        base = F.cross_entropy(logits[mask], labels[mask]) if mask.any() else torch.tensor(0.0, device=logits.device)\n",
    "        l_lex = lexical_diversity_loss(labels.view(1, -1), pad_token_id)\n",
    "        l_contra = nli_contradiction_loss(body1_text, body2_text) if body1_text is not None else 0.0\n",
    "        l_topic = topic_relevance_loss(topic_text, body2_text) * 0.9 if topic_text is not None else 0.0\n",
    "        l_rep = repetition_overlap_loss(body1_text, body2_text) if body1_text is not None else 0.0\n",
    "        l_ngram = ngram_overlap_loss(body2_text) if body2_text is not None else 0.0\n",
    "        l_arg = argument_distance_loss(body1_text, body2_text) if body1_text is not None else 0.0\n",
    "        l_sem = semantic_similarity_loss(intros, body2_text) if intros is not None and body2_text is not None else 0.0\n",
    "        l_gen_b2 = argument_presence_loss(body2_text) if body2_text is not None else 0.0\n",
    "        l_gen_b1 = argument_presence_loss(body1_text) if body1_text is not None else 0.0\n",
    "        return base + 0.2 * l_lex + 0.7 * l_contra + 0.9 * l_topic + 0.5 * l_rep + 0.4 * l_ngram + 0.4 * l_arg + 0.4 * l_sem + 0.5 * l_gen_b2 + 0.3 * l_gen_b1\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            intros = inputs.get(\"intro\")\n",
    "            body1_text = inputs.get(\"body1_text\")\n",
    "            body2_text = inputs.get(\"body2_text\")\n",
    "            topic_text = inputs.get(\"topic\")\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], labels=labels)\n",
    "            loss = dpo_loss(outputs.logits, labels, intros, body1_text, body2_text, topic_text)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=save_dir,\n",
    "        report_to=[\"wandb\"],\n",
    "        run_name=f\"{field}_v24\",\n",
    "        logging_dir=f\"{save_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=4,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized[\"train\"],\n",
    "        eval_dataset=tokenized[\"test\"],\n",
    "        compute_metrics=lambda p: compute_metrics(p,lam = 0.5)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(save_dir)\n",
    "    tokenizer.save_pretrained(save_dir)\n",
    "    print(f\"✅ Saved model to: {save_dir}\", flush=True)\n",
    "\n",
    "# === Train all paragraph models for v24 ===\n",
    "train_paragraph_model(\"intro\", \"./t5_intro_lora_v24\", max_target_length=160)\n",
    "train_paragraph_model(\"body1\", \"./t5_body1_lora_v24\", max_target_length=240)\n",
    "train_paragraph_model(\"body2\", \"./t5_body2_lora_v24\", max_target_length=288)\n",
    "train_paragraph_model(\"conclusion\", \"./t5_conclusion_lora_v24\", max_target_length=96)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
