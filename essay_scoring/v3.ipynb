{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9517c4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# === 1. Imports & dataset loading ===\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import mse_loss\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# === 2. Load & preprocess dataset ===\n",
    "dataset = load_dataset(\"chillies/IELTS-writing-task-2-evaluation\")\n",
    "\n",
    "def extract_scores(evaluation_text):\n",
    "    criteria = {\n",
    "        \"task_score\": r\"Task Achievement.*?\\[(\\d+\\.?\\d*)\\]|Suggested Band Score \\(Task Achievement\\): (\\d+\\.?\\d*)\",\n",
    "        \"coherence_score\": r\"Coherence and Cohesion.*?\\[(\\d+\\.?\\d*)\\]|Suggested Band Score \\(Coherence and Cohesion\\): (\\d+\\.?\\d*)\",\n",
    "        \"lexical_score\": r\"Lexical Resource.*?\\[(\\d+\\.?\\d*)\\]|Suggested Band Score \\(Lexical Resource\\): (\\d+\\.?\\d*)\",\n",
    "        \"grammar_score\": r\"Grammatical Range and Accuracy.*?\\[(\\d+\\.?\\d*)\\]|Suggested Band Score \\(Grammatical Range and Accuracy\\): (\\d+\\.?\\d*)\",\n",
    "    }\n",
    "    scores = {}\n",
    "    for key, pattern in criteria.items():\n",
    "        match = re.search(pattern, evaluation_text, re.DOTALL)\n",
    "        if match:\n",
    "            scores[key] = float(match.group(1) or match.group(2))\n",
    "        else:\n",
    "            return None\n",
    "    return scores\n",
    "\n",
    "def preprocess(example):\n",
    "    scores = extract_scores(example[\"evaluation\"])\n",
    "    try:\n",
    "        overall_band = float(re.sub(r\"[^\\d.]\", \"\", example[\"band\"]))\n",
    "    except:\n",
    "        overall_band = None\n",
    "    if scores and overall_band:\n",
    "        return {\n",
    "            \"prompt\": example[\"prompt\"],\n",
    "            \"essay\": example[\"essay\"],\n",
    "            **scores,\n",
    "            \"overall_score\": overall_band\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"prompt\": None,\n",
    "            \"essay\": None,\n",
    "            \"task_score\": None,\n",
    "            \"coherence_score\": None,\n",
    "            \"lexical_score\": None,\n",
    "            \"grammar_score\": None,\n",
    "            \"overall_score\": None\n",
    "        }\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "dataset = dataset.filter(lambda x: x[\"prompt\"] is not None)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === 3. Tokenization & normalization ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    text = f\"Prompt: {example['prompt']}\\nEssay: {example['essay']}\"\n",
    "    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    # Normalize label: (band - 4) / 5 â†’ [0, 1] For: 4-4=0/5=0, 9-4=5/5=1\n",
    "    tokenized[\"labels\"] = [\n",
    "        (example[\"task_score\"] - 4) / 5,\n",
    "        (example[\"coherence_score\"] - 4) / 5,\n",
    "        (example[\"lexical_score\"] - 4) / 5,\n",
    "        (example[\"grammar_score\"] - 4) / 5,\n",
    "        (example[\"overall_score\"] - 4) / 5,\n",
    "    ]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function)\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:519: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Trainable params: 446,213 / 184,277,765 (0.24%)\n"
     ]
    }
   ],
   "source": [
    "# === 4. Model + LoRA ===\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForMaskedLM.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "\n",
    "class DebertaWithLoRA(nn.Module):\n",
    "    def __init__(self, base_model, lora_config):\n",
    "        super().__init__()\n",
    "        self.backbone = get_peft_model(base_model.base_model, lora_config)\n",
    "        self.regressor = nn.Linear(base_model.config.hidden_size, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        if \"labels\" in kwargs:\n",
    "            kwargs.pop(\"labels\")\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.regressor(cls_output)\n",
    "\n",
    "model = DebertaWithLoRA(base_model, lora_config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def print_trainable_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"ðŸ”§ Trainable params: {trainable:,} / {total:,} ({100 * trainable / total:.2f}%)\")\n",
    "\n",
    "print_trainable_params(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6525277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: st124689 (binit-ait) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\VUONGLOCTRUONG\\Downloads\\quiz2\\essay_scoring\\wandb\\run-20250414_141211-gb9g7bf0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binit-ait/huggingface/runs/gb9g7bf0' target=\"_blank\">iconic-sea-159</a></strong> to <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binit-ait/huggingface' target=\"_blank\">https://wandb.ai/binit-ait/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binit-ait/huggingface/runs/gb9g7bf0' target=\"_blank\">https://wandb.ai/binit-ait/huggingface/runs/gb9g7bf0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cc56c4fbf6407ea41d7172013ce29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3783, 'learning_rate': 0.00019964404894327032, 'epoch': 0.01}\n",
      "{'loss': 0.2104, 'learning_rate': 0.0001991991101223582, 'epoch': 0.02}\n",
      "{'loss': 0.1529, 'learning_rate': 0.00019875417130144608, 'epoch': 0.03}\n",
      "{'loss': 0.105, 'learning_rate': 0.00019830923248053396, 'epoch': 0.04}\n",
      "{'loss': 0.1367, 'learning_rate': 0.0001978642936596218, 'epoch': 0.06}\n",
      "{'loss': 0.0975, 'learning_rate': 0.00019741935483870969, 'epoch': 0.07}\n",
      "{'loss': 0.1639, 'learning_rate': 0.00019697441601779756, 'epoch': 0.08}\n",
      "{'loss': 0.1207, 'learning_rate': 0.00019652947719688544, 'epoch': 0.09}\n",
      "{'loss': 0.1085, 'learning_rate': 0.00019608453837597332, 'epoch': 0.1}\n",
      "{'loss': 0.1446, 'learning_rate': 0.0001956395995550612, 'epoch': 0.11}\n",
      "{'loss': 0.1137, 'learning_rate': 0.00019519466073414908, 'epoch': 0.12}\n",
      "{'loss': 0.1311, 'learning_rate': 0.00019474972191323696, 'epoch': 0.13}\n",
      "{'loss': 0.1196, 'learning_rate': 0.0001943047830923248, 'epoch': 0.14}\n",
      "{'loss': 0.0686, 'learning_rate': 0.00019385984427141268, 'epoch': 0.16}\n",
      "{'loss': 0.0983, 'learning_rate': 0.00019341490545050056, 'epoch': 0.17}\n",
      "{'loss': 0.0726, 'learning_rate': 0.00019296996662958844, 'epoch': 0.18}\n",
      "{'loss': 0.0835, 'learning_rate': 0.00019252502780867632, 'epoch': 0.19}\n",
      "{'loss': 0.0969, 'learning_rate': 0.0001920800889877642, 'epoch': 0.2}\n",
      "{'loss': 0.079, 'learning_rate': 0.00019163515016685207, 'epoch': 0.21}\n",
      "{'loss': 0.0902, 'learning_rate': 0.00019119021134593995, 'epoch': 0.22}\n",
      "{'loss': 0.1028, 'learning_rate': 0.0001907452725250278, 'epoch': 0.23}\n",
      "{'loss': 0.0813, 'learning_rate': 0.00019030033370411568, 'epoch': 0.24}\n",
      "{'loss': 0.1023, 'learning_rate': 0.00018985539488320356, 'epoch': 0.26}\n",
      "{'loss': 0.1392, 'learning_rate': 0.00018941045606229144, 'epoch': 0.27}\n",
      "{'loss': 0.0923, 'learning_rate': 0.00018896551724137932, 'epoch': 0.28}\n",
      "{'loss': 0.0993, 'learning_rate': 0.0001885205784204672, 'epoch': 0.29}\n",
      "{'loss': 0.085, 'learning_rate': 0.00018807563959955507, 'epoch': 0.3}\n",
      "{'loss': 0.0589, 'learning_rate': 0.00018763070077864295, 'epoch': 0.31}\n",
      "{'loss': 0.1115, 'learning_rate': 0.00018718576195773083, 'epoch': 0.32}\n",
      "{'loss': 0.071, 'learning_rate': 0.00018674082313681868, 'epoch': 0.33}\n",
      "{'loss': 0.1026, 'learning_rate': 0.00018629588431590656, 'epoch': 0.34}\n",
      "{'loss': 0.0969, 'learning_rate': 0.00018585094549499444, 'epoch': 0.36}\n",
      "{'loss': 0.1061, 'learning_rate': 0.00018540600667408231, 'epoch': 0.37}\n",
      "{'loss': 0.0679, 'learning_rate': 0.0001849610678531702, 'epoch': 0.38}\n",
      "{'loss': 0.0846, 'learning_rate': 0.00018451612903225807, 'epoch': 0.39}\n",
      "{'loss': 0.0989, 'learning_rate': 0.00018407119021134595, 'epoch': 0.4}\n",
      "{'loss': 0.0847, 'learning_rate': 0.00018362625139043383, 'epoch': 0.41}\n",
      "{'loss': 0.0868, 'learning_rate': 0.00018318131256952168, 'epoch': 0.42}\n",
      "{'loss': 0.0976, 'learning_rate': 0.00018273637374860956, 'epoch': 0.43}\n",
      "{'loss': 0.0626, 'learning_rate': 0.00018229143492769743, 'epoch': 0.44}\n",
      "{'loss': 0.0925, 'learning_rate': 0.0001818464961067853, 'epoch': 0.46}\n",
      "{'loss': 0.0958, 'learning_rate': 0.0001814015572858732, 'epoch': 0.47}\n",
      "{'loss': 0.0735, 'learning_rate': 0.00018095661846496107, 'epoch': 0.48}\n",
      "{'loss': 0.0847, 'learning_rate': 0.00018051167964404895, 'epoch': 0.49}\n",
      "{'loss': 0.0643, 'learning_rate': 0.00018006674082313682, 'epoch': 0.5}\n",
      "{'loss': 0.0741, 'learning_rate': 0.0001796218020022247, 'epoch': 0.51}\n",
      "{'loss': 0.1008, 'learning_rate': 0.00017917686318131258, 'epoch': 0.52}\n",
      "{'loss': 0.0711, 'learning_rate': 0.00017873192436040046, 'epoch': 0.53}\n",
      "{'loss': 0.0714, 'learning_rate': 0.00017828698553948834, 'epoch': 0.55}\n",
      "{'loss': 0.0562, 'learning_rate': 0.00017784204671857621, 'epoch': 0.56}\n",
      "{'loss': 0.1095, 'learning_rate': 0.0001773971078976641, 'epoch': 0.57}\n",
      "{'loss': 0.1054, 'learning_rate': 0.00017695216907675197, 'epoch': 0.58}\n",
      "{'loss': 0.0774, 'learning_rate': 0.00017650723025583985, 'epoch': 0.59}\n",
      "{'loss': 0.0763, 'learning_rate': 0.0001760622914349277, 'epoch': 0.6}\n",
      "{'loss': 0.0975, 'learning_rate': 0.00017561735261401558, 'epoch': 0.61}\n",
      "{'loss': 0.0652, 'learning_rate': 0.00017517241379310346, 'epoch': 0.62}\n",
      "{'loss': 0.0926, 'learning_rate': 0.00017472747497219133, 'epoch': 0.63}\n",
      "{'loss': 0.0856, 'learning_rate': 0.0001742825361512792, 'epoch': 0.65}\n",
      "{'loss': 0.0842, 'learning_rate': 0.0001738375973303671, 'epoch': 0.66}\n",
      "{'loss': 0.0855, 'learning_rate': 0.00017339265850945497, 'epoch': 0.67}\n",
      "{'loss': 0.077, 'learning_rate': 0.00017294771968854285, 'epoch': 0.68}\n",
      "{'loss': 0.0843, 'learning_rate': 0.00017250278086763072, 'epoch': 0.69}\n",
      "{'loss': 0.0591, 'learning_rate': 0.00017205784204671858, 'epoch': 0.7}\n",
      "{'loss': 0.0699, 'learning_rate': 0.00017161290322580645, 'epoch': 0.71}\n",
      "{'loss': 0.0821, 'learning_rate': 0.00017116796440489433, 'epoch': 0.72}\n",
      "{'loss': 0.0739, 'learning_rate': 0.0001707230255839822, 'epoch': 0.73}\n",
      "{'loss': 0.0845, 'learning_rate': 0.0001702780867630701, 'epoch': 0.75}\n",
      "{'loss': 0.0862, 'learning_rate': 0.00016983314794215797, 'epoch': 0.76}\n",
      "{'loss': 0.0811, 'learning_rate': 0.00016938820912124584, 'epoch': 0.77}\n",
      "{'loss': 0.1026, 'learning_rate': 0.00016894327030033372, 'epoch': 0.78}\n",
      "{'loss': 0.0636, 'learning_rate': 0.00016849833147942157, 'epoch': 0.79}\n",
      "{'loss': 0.1097, 'learning_rate': 0.00016805339265850945, 'epoch': 0.8}\n",
      "{'loss': 0.0845, 'learning_rate': 0.00016760845383759733, 'epoch': 0.81}\n",
      "{'loss': 0.0827, 'learning_rate': 0.0001671635150166852, 'epoch': 0.82}\n",
      "{'loss': 0.0643, 'learning_rate': 0.00016671857619577309, 'epoch': 0.83}\n",
      "{'loss': 0.0591, 'learning_rate': 0.00016627363737486096, 'epoch': 0.85}\n",
      "{'loss': 0.071, 'learning_rate': 0.00016582869855394884, 'epoch': 0.86}\n",
      "{'loss': 0.0667, 'learning_rate': 0.00016538375973303672, 'epoch': 0.87}\n",
      "{'loss': 0.0542, 'learning_rate': 0.00016493882091212457, 'epoch': 0.88}\n",
      "{'loss': 0.0783, 'learning_rate': 0.00016449388209121245, 'epoch': 0.89}\n",
      "{'loss': 0.0881, 'learning_rate': 0.00016404894327030033, 'epoch': 0.9}\n",
      "{'loss': 0.0817, 'learning_rate': 0.0001636040044493882, 'epoch': 0.91}\n",
      "{'loss': 0.0775, 'learning_rate': 0.00016315906562847608, 'epoch': 0.92}\n",
      "{'loss': 0.0992, 'learning_rate': 0.00016271412680756396, 'epoch': 0.93}\n",
      "{'loss': 0.0609, 'learning_rate': 0.00016226918798665184, 'epoch': 0.95}\n",
      "{'loss': 0.0761, 'learning_rate': 0.00016182424916573972, 'epoch': 0.96}\n",
      "{'loss': 0.0753, 'learning_rate': 0.0001613793103448276, 'epoch': 0.97}\n",
      "{'loss': 0.0772, 'learning_rate': 0.00016093437152391547, 'epoch': 0.98}\n",
      "{'loss': 0.0835, 'learning_rate': 0.00016048943270300335, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020aed940f42476ea4bb0d57839d6955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª compute_metrics called\n",
      "{'eval_RMSE': 0.26052042841911316, 'eval_loss': 0.06787089258432388, 'eval_runtime': 23.8942, 'eval_samples_per_second': 16.74, 'eval_steps_per_second': 4.185, 'epoch': 1.0}\n",
      "{'loss': 0.0831, 'learning_rate': 0.00016004449388209123, 'epoch': 1.0}\n",
      "{'loss': 0.0767, 'learning_rate': 0.0001595995550611791, 'epoch': 1.01}\n",
      "{'loss': 0.0726, 'learning_rate': 0.000159154616240267, 'epoch': 1.02}\n",
      "{'loss': 0.0689, 'learning_rate': 0.00015870967741935487, 'epoch': 1.03}\n",
      "{'loss': 0.0563, 'learning_rate': 0.00015826473859844274, 'epoch': 1.05}\n",
      "{'loss': 0.0968, 'learning_rate': 0.00015781979977753062, 'epoch': 1.06}\n",
      "{'loss': 0.075, 'learning_rate': 0.00015737486095661847, 'epoch': 1.07}\n",
      "{'loss': 0.0903, 'learning_rate': 0.00015692992213570635, 'epoch': 1.08}\n",
      "{'loss': 0.0468, 'learning_rate': 0.00015648498331479423, 'epoch': 1.09}\n",
      "{'loss': 0.0952, 'learning_rate': 0.0001560400444938821, 'epoch': 1.1}\n",
      "{'loss': 0.0714, 'learning_rate': 0.00015559510567296998, 'epoch': 1.11}\n",
      "{'loss': 0.0726, 'learning_rate': 0.00015515016685205786, 'epoch': 1.12}\n",
      "{'loss': 0.0738, 'learning_rate': 0.00015470522803114574, 'epoch': 1.13}\n",
      "{'loss': 0.0678, 'learning_rate': 0.00015426028921023362, 'epoch': 1.15}\n",
      "{'loss': 0.0755, 'learning_rate': 0.00015381535038932147, 'epoch': 1.16}\n",
      "{'loss': 0.059, 'learning_rate': 0.00015337041156840935, 'epoch': 1.17}\n",
      "{'loss': 0.0735, 'learning_rate': 0.00015292547274749723, 'epoch': 1.18}\n",
      "{'loss': 0.0751, 'learning_rate': 0.0001524805339265851, 'epoch': 1.19}\n",
      "{'loss': 0.0898, 'learning_rate': 0.00015203559510567298, 'epoch': 1.2}\n",
      "{'loss': 0.0828, 'learning_rate': 0.00015159065628476086, 'epoch': 1.21}\n",
      "{'loss': 0.0817, 'learning_rate': 0.00015114571746384874, 'epoch': 1.22}\n",
      "{'loss': 0.0609, 'learning_rate': 0.00015070077864293662, 'epoch': 1.23}\n",
      "{'loss': 0.0586, 'learning_rate': 0.00015025583982202447, 'epoch': 1.25}\n",
      "{'loss': 0.0585, 'learning_rate': 0.00014981090100111235, 'epoch': 1.26}\n",
      "{'loss': 0.078, 'learning_rate': 0.00014936596218020022, 'epoch': 1.27}\n",
      "{'loss': 0.0662, 'learning_rate': 0.0001489210233592881, 'epoch': 1.28}\n",
      "{'loss': 0.0608, 'learning_rate': 0.00014847608453837598, 'epoch': 1.29}\n",
      "{'loss': 0.074, 'learning_rate': 0.00014803114571746386, 'epoch': 1.3}\n",
      "{'loss': 0.0793, 'learning_rate': 0.00014758620689655174, 'epoch': 1.31}\n",
      "{'loss': 0.0653, 'learning_rate': 0.00014714126807563961, 'epoch': 1.32}\n",
      "{'loss': 0.0735, 'learning_rate': 0.00014669632925472747, 'epoch': 1.33}\n",
      "{'loss': 0.0779, 'learning_rate': 0.00014625139043381534, 'epoch': 1.35}\n",
      "{'loss': 0.0491, 'learning_rate': 0.00014580645161290322, 'epoch': 1.36}\n",
      "{'loss': 0.0691, 'learning_rate': 0.0001453615127919911, 'epoch': 1.37}\n",
      "{'loss': 0.0738, 'learning_rate': 0.00014491657397107898, 'epoch': 1.38}\n",
      "{'loss': 0.0704, 'learning_rate': 0.00014447163515016686, 'epoch': 1.39}\n",
      "{'loss': 0.0613, 'learning_rate': 0.00014402669632925473, 'epoch': 1.4}\n",
      "{'loss': 0.0475, 'learning_rate': 0.0001435817575083426, 'epoch': 1.41}\n",
      "{'loss': 0.0945, 'learning_rate': 0.0001431368186874305, 'epoch': 1.42}\n",
      "{'loss': 0.0808, 'learning_rate': 0.00014269187986651834, 'epoch': 1.43}\n",
      "{'loss': 0.1005, 'learning_rate': 0.00014224694104560622, 'epoch': 1.45}\n",
      "{'loss': 0.0833, 'learning_rate': 0.0001418020022246941, 'epoch': 1.46}\n",
      "{'loss': 0.094, 'learning_rate': 0.00014135706340378198, 'epoch': 1.47}\n",
      "{'loss': 0.071, 'learning_rate': 0.00014091212458286985, 'epoch': 1.48}\n",
      "{'loss': 0.066, 'learning_rate': 0.00014046718576195773, 'epoch': 1.49}\n",
      "{'loss': 0.0519, 'learning_rate': 0.0001400222469410456, 'epoch': 1.5}\n",
      "{'loss': 0.0694, 'learning_rate': 0.0001395773081201335, 'epoch': 1.51}\n",
      "{'loss': 0.0836, 'learning_rate': 0.00013913236929922137, 'epoch': 1.52}\n",
      "{'loss': 0.0483, 'learning_rate': 0.00013868743047830924, 'epoch': 1.54}\n",
      "{'loss': 0.0724, 'learning_rate': 0.00013824249165739712, 'epoch': 1.55}\n",
      "{'loss': 0.0568, 'learning_rate': 0.000137797552836485, 'epoch': 1.56}\n",
      "{'loss': 0.0425, 'learning_rate': 0.00013735261401557288, 'epoch': 1.57}\n",
      "{'loss': 0.0673, 'learning_rate': 0.00013690767519466076, 'epoch': 1.58}\n",
      "{'loss': 0.0403, 'learning_rate': 0.00013646273637374863, 'epoch': 1.59}\n",
      "{'loss': 0.0778, 'learning_rate': 0.0001360177975528365, 'epoch': 1.6}\n",
      "{'loss': 0.0757, 'learning_rate': 0.00013557285873192436, 'epoch': 1.61}\n",
      "{'loss': 0.0746, 'learning_rate': 0.00013512791991101224, 'epoch': 1.62}\n",
      "{'loss': 0.0519, 'learning_rate': 0.00013468298109010012, 'epoch': 1.64}\n",
      "{'loss': 0.0465, 'learning_rate': 0.000134238042269188, 'epoch': 1.65}\n",
      "{'loss': 0.0706, 'learning_rate': 0.00013379310344827588, 'epoch': 1.66}\n",
      "{'loss': 0.0753, 'learning_rate': 0.00013334816462736375, 'epoch': 1.67}\n",
      "{'loss': 0.0525, 'learning_rate': 0.00013290322580645163, 'epoch': 1.68}\n",
      "{'loss': 0.0497, 'learning_rate': 0.0001324582869855395, 'epoch': 1.69}\n",
      "{'loss': 0.0751, 'learning_rate': 0.00013201334816462736, 'epoch': 1.7}\n",
      "{'loss': 0.094, 'learning_rate': 0.00013156840934371524, 'epoch': 1.71}\n",
      "{'loss': 0.0875, 'learning_rate': 0.00013112347052280312, 'epoch': 1.72}\n",
      "{'loss': 0.0518, 'learning_rate': 0.000130678531701891, 'epoch': 1.74}\n",
      "{'loss': 0.0697, 'learning_rate': 0.00013023359288097887, 'epoch': 1.75}\n",
      "{'loss': 0.0731, 'learning_rate': 0.00012978865406006675, 'epoch': 1.76}\n",
      "{'loss': 0.0604, 'learning_rate': 0.00012934371523915463, 'epoch': 1.77}\n",
      "{'loss': 0.0826, 'learning_rate': 0.0001288987764182425, 'epoch': 1.78}\n",
      "{'loss': 0.0711, 'learning_rate': 0.0001284538375973304, 'epoch': 1.79}\n",
      "{'loss': 0.0417, 'learning_rate': 0.00012800889877641824, 'epoch': 1.8}\n",
      "{'loss': 0.0437, 'learning_rate': 0.00012756395995550612, 'epoch': 1.81}\n",
      "{'loss': 0.0662, 'learning_rate': 0.000127119021134594, 'epoch': 1.82}\n",
      "{'loss': 0.0625, 'learning_rate': 0.00012667408231368187, 'epoch': 1.84}\n",
      "{'loss': 0.0608, 'learning_rate': 0.00012622914349276975, 'epoch': 1.85}\n",
      "{'loss': 0.0665, 'learning_rate': 0.00012578420467185763, 'epoch': 1.86}\n",
      "{'loss': 0.0515, 'learning_rate': 0.0001253392658509455, 'epoch': 1.87}\n",
      "{'loss': 0.0506, 'learning_rate': 0.00012489432703003338, 'epoch': 1.88}\n",
      "{'loss': 0.0934, 'learning_rate': 0.00012444938820912124, 'epoch': 1.89}\n",
      "{'loss': 0.0749, 'learning_rate': 0.00012400444938820911, 'epoch': 1.9}\n",
      "{'loss': 0.064, 'learning_rate': 0.000123559510567297, 'epoch': 1.91}\n",
      "{'loss': 0.0586, 'learning_rate': 0.00012311457174638487, 'epoch': 1.92}\n",
      "{'loss': 0.0468, 'learning_rate': 0.00012266963292547275, 'epoch': 1.94}\n",
      "{'loss': 0.0642, 'learning_rate': 0.00012222469410456063, 'epoch': 1.95}\n",
      "{'loss': 0.0771, 'learning_rate': 0.00012177975528364852, 'epoch': 1.96}\n",
      "{'loss': 0.0659, 'learning_rate': 0.0001213348164627364, 'epoch': 1.97}\n",
      "{'loss': 0.0484, 'learning_rate': 0.00012088987764182425, 'epoch': 1.98}\n",
      "{'loss': 0.0789, 'learning_rate': 0.00012044493882091212, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313fb168c3a749728b19f1e7803cb6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª compute_metrics called\n",
      "{'eval_RMSE': 0.2428446263074875, 'eval_loss': 0.058973513543605804, 'eval_runtime': 15.8831, 'eval_samples_per_second': 25.184, 'eval_steps_per_second': 6.296, 'epoch': 2.0}\n",
      "{'loss': 0.0625, 'learning_rate': 0.00012, 'epoch': 2.0}\n",
      "{'loss': 0.0368, 'learning_rate': 0.00011955506117908788, 'epoch': 2.01}\n",
      "{'loss': 0.0626, 'learning_rate': 0.00011911012235817576, 'epoch': 2.02}\n",
      "{'loss': 0.0479, 'learning_rate': 0.00011866518353726364, 'epoch': 2.04}\n",
      "{'loss': 0.0756, 'learning_rate': 0.00011822024471635152, 'epoch': 2.05}\n",
      "{'loss': 0.103, 'learning_rate': 0.0001177753058954394, 'epoch': 2.06}\n",
      "{'loss': 0.0594, 'learning_rate': 0.00011733036707452724, 'epoch': 2.07}\n",
      "{'loss': 0.0872, 'learning_rate': 0.00011688542825361512, 'epoch': 2.08}\n",
      "{'loss': 0.0779, 'learning_rate': 0.000116440489432703, 'epoch': 2.09}\n",
      "{'loss': 0.0534, 'learning_rate': 0.00011599555061179088, 'epoch': 2.1}\n",
      "{'loss': 0.04, 'learning_rate': 0.00011555061179087876, 'epoch': 2.11}\n",
      "{'loss': 0.0583, 'learning_rate': 0.00011510567296996664, 'epoch': 2.12}\n",
      "{'loss': 0.0581, 'learning_rate': 0.00011466073414905451, 'epoch': 2.14}\n",
      "{'loss': 0.0601, 'learning_rate': 0.00011421579532814239, 'epoch': 2.15}\n",
      "{'loss': 0.044, 'learning_rate': 0.00011377085650723027, 'epoch': 2.16}\n",
      "{'loss': 0.0809, 'learning_rate': 0.00011332591768631813, 'epoch': 2.17}\n",
      "{'loss': 0.052, 'learning_rate': 0.00011288097886540601, 'epoch': 2.18}\n",
      "{'loss': 0.0504, 'learning_rate': 0.00011243604004449389, 'epoch': 2.19}\n",
      "{'loss': 0.0703, 'learning_rate': 0.00011199110122358177, 'epoch': 2.2}\n",
      "{'loss': 0.0726, 'learning_rate': 0.00011154616240266965, 'epoch': 2.21}\n",
      "{'loss': 0.0753, 'learning_rate': 0.00011110122358175752, 'epoch': 2.22}\n",
      "{'loss': 0.1, 'learning_rate': 0.0001106562847608454, 'epoch': 2.24}\n",
      "{'loss': 0.0813, 'learning_rate': 0.00011021134593993328, 'epoch': 2.25}\n",
      "{'loss': 0.0911, 'learning_rate': 0.00010976640711902113, 'epoch': 2.26}\n",
      "{'loss': 0.0744, 'learning_rate': 0.00010932146829810901, 'epoch': 2.27}\n",
      "{'loss': 0.0664, 'learning_rate': 0.00010887652947719689, 'epoch': 2.28}\n",
      "{'loss': 0.0593, 'learning_rate': 0.00010843159065628477, 'epoch': 2.29}\n",
      "{'loss': 0.0605, 'learning_rate': 0.00010798665183537264, 'epoch': 2.3}\n",
      "{'loss': 0.0599, 'learning_rate': 0.00010754171301446052, 'epoch': 2.31}\n",
      "{'loss': 0.0413, 'learning_rate': 0.0001070967741935484, 'epoch': 2.32}\n",
      "{'loss': 0.0635, 'learning_rate': 0.00010665183537263628, 'epoch': 2.34}\n",
      "{'loss': 0.0566, 'learning_rate': 0.00010620689655172413, 'epoch': 2.35}\n",
      "{'loss': 0.0629, 'learning_rate': 0.00010576195773081201, 'epoch': 2.36}\n",
      "{'loss': 0.0706, 'learning_rate': 0.00010531701890989989, 'epoch': 2.37}\n",
      "{'loss': 0.047, 'learning_rate': 0.00010487208008898776, 'epoch': 2.38}\n",
      "{'loss': 0.0519, 'learning_rate': 0.00010442714126807564, 'epoch': 2.39}\n",
      "{'loss': 0.07, 'learning_rate': 0.00010398220244716352, 'epoch': 2.4}\n",
      "{'loss': 0.0537, 'learning_rate': 0.0001035372636262514, 'epoch': 2.41}\n",
      "{'loss': 0.0696, 'learning_rate': 0.00010309232480533928, 'epoch': 2.42}\n",
      "{'loss': 0.0517, 'learning_rate': 0.00010264738598442714, 'epoch': 2.44}\n",
      "{'loss': 0.0493, 'learning_rate': 0.00010220244716351502, 'epoch': 2.45}\n",
      "{'loss': 0.0653, 'learning_rate': 0.0001017575083426029, 'epoch': 2.46}\n",
      "{'loss': 0.0736, 'learning_rate': 0.00010131256952169078, 'epoch': 2.47}\n",
      "{'loss': 0.046, 'learning_rate': 0.00010086763070077865, 'epoch': 2.48}\n",
      "{'loss': 0.0325, 'learning_rate': 0.00010042269187986653, 'epoch': 2.49}\n",
      "{'loss': 0.0706, 'learning_rate': 9.99777530589544e-05, 'epoch': 2.5}\n",
      "{'loss': 0.0833, 'learning_rate': 9.953281423804227e-05, 'epoch': 2.51}\n",
      "{'loss': 0.0517, 'learning_rate': 9.908787541713015e-05, 'epoch': 2.53}\n",
      "{'loss': 0.0619, 'learning_rate': 9.864293659621803e-05, 'epoch': 2.54}\n",
      "{'loss': 0.0369, 'learning_rate': 9.819799777530591e-05, 'epoch': 2.55}\n",
      "{'loss': 0.0736, 'learning_rate': 9.775305895439377e-05, 'epoch': 2.56}\n",
      "{'loss': 0.0873, 'learning_rate': 9.730812013348165e-05, 'epoch': 2.57}\n",
      "{'loss': 0.0657, 'learning_rate': 9.686318131256953e-05, 'epoch': 2.58}\n",
      "{'loss': 0.099, 'learning_rate': 9.641824249165741e-05, 'epoch': 2.59}\n",
      "{'loss': 0.0844, 'learning_rate': 9.597330367074527e-05, 'epoch': 2.6}\n",
      "{'loss': 0.0503, 'learning_rate': 9.552836484983315e-05, 'epoch': 2.61}\n",
      "{'loss': 0.0569, 'learning_rate': 9.508342602892103e-05, 'epoch': 2.63}\n",
      "{'loss': 0.0844, 'learning_rate': 9.46384872080089e-05, 'epoch': 2.64}\n",
      "{'loss': 0.0599, 'learning_rate': 9.419354838709677e-05, 'epoch': 2.65}\n",
      "{'loss': 0.0381, 'learning_rate': 9.374860956618465e-05, 'epoch': 2.66}\n",
      "{'loss': 0.0749, 'learning_rate': 9.330367074527253e-05, 'epoch': 2.67}\n",
      "{'loss': 0.0603, 'learning_rate': 9.28587319243604e-05, 'epoch': 2.68}\n",
      "{'loss': 0.0495, 'learning_rate': 9.241379310344827e-05, 'epoch': 2.69}\n",
      "{'loss': 0.0764, 'learning_rate': 9.196885428253615e-05, 'epoch': 2.7}\n",
      "{'loss': 0.0353, 'learning_rate': 9.152391546162403e-05, 'epoch': 2.71}\n",
      "{'loss': 0.0739, 'learning_rate': 9.10789766407119e-05, 'epoch': 2.73}\n",
      "{'loss': 0.0514, 'learning_rate': 9.063403781979978e-05, 'epoch': 2.74}\n",
      "{'loss': 0.0632, 'learning_rate': 9.018909899888766e-05, 'epoch': 2.75}\n",
      "{'loss': 0.0484, 'learning_rate': 8.974416017797554e-05, 'epoch': 2.76}\n",
      "{'loss': 0.0455, 'learning_rate': 8.929922135706342e-05, 'epoch': 2.77}\n",
      "{'loss': 0.0464, 'learning_rate': 8.885428253615128e-05, 'epoch': 2.78}\n",
      "{'loss': 0.0739, 'learning_rate': 8.840934371523916e-05, 'epoch': 2.79}\n",
      "{'loss': 0.0372, 'learning_rate': 8.796440489432704e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0602, 'learning_rate': 8.751946607341492e-05, 'epoch': 2.81}\n",
      "{'loss': 0.0509, 'learning_rate': 8.707452725250278e-05, 'epoch': 2.83}\n",
      "{'loss': 0.0667, 'learning_rate': 8.662958843159066e-05, 'epoch': 2.84}\n",
      "{'loss': 0.0648, 'learning_rate': 8.618464961067854e-05, 'epoch': 2.85}\n",
      "{'loss': 0.0637, 'learning_rate': 8.573971078976641e-05, 'epoch': 2.86}\n",
      "{'loss': 0.0507, 'learning_rate': 8.529477196885428e-05, 'epoch': 2.87}\n",
      "{'loss': 0.0679, 'learning_rate': 8.484983314794216e-05, 'epoch': 2.88}\n",
      "{'loss': 0.0611, 'learning_rate': 8.440489432703003e-05, 'epoch': 2.89}\n",
      "{'loss': 0.0529, 'learning_rate': 8.395995550611791e-05, 'epoch': 2.9}\n",
      "{'loss': 0.0386, 'learning_rate': 8.351501668520579e-05, 'epoch': 2.91}\n",
      "{'loss': 0.0402, 'learning_rate': 8.307007786429366e-05, 'epoch': 2.93}\n",
      "{'loss': 0.0708, 'learning_rate': 8.262513904338153e-05, 'epoch': 2.94}\n",
      "{'loss': 0.0729, 'learning_rate': 8.218020022246941e-05, 'epoch': 2.95}\n",
      "{'loss': 0.0745, 'learning_rate': 8.173526140155729e-05, 'epoch': 2.96}\n",
      "{'loss': 0.0681, 'learning_rate': 8.129032258064517e-05, 'epoch': 2.97}\n",
      "{'loss': 0.0746, 'learning_rate': 8.084538375973305e-05, 'epoch': 2.98}\n",
      "{'loss': 0.0428, 'learning_rate': 8.040044493882092e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62263b86bf74c28884f3d75282c6209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª compute_metrics called\n",
      "{'eval_RMSE': 0.23546022176742554, 'eval_loss': 0.055441513657569885, 'eval_runtime': 15.8705, 'eval_samples_per_second': 25.204, 'eval_steps_per_second': 6.301, 'epoch': 3.0}\n",
      "{'loss': 0.0462, 'learning_rate': 7.99555061179088e-05, 'epoch': 3.0}\n",
      "{'loss': 0.0491, 'learning_rate': 7.951056729699667e-05, 'epoch': 3.01}\n",
      "{'loss': 0.0541, 'learning_rate': 7.906562847608455e-05, 'epoch': 3.03}\n",
      "{'loss': 0.055, 'learning_rate': 7.862068965517242e-05, 'epoch': 3.04}\n",
      "{'loss': 0.0486, 'learning_rate': 7.81757508342603e-05, 'epoch': 3.05}\n",
      "{'loss': 0.0636, 'learning_rate': 7.773081201334817e-05, 'epoch': 3.06}\n",
      "{'loss': 0.0592, 'learning_rate': 7.728587319243604e-05, 'epoch': 3.07}\n",
      "{'loss': 0.0585, 'learning_rate': 7.684093437152392e-05, 'epoch': 3.08}\n",
      "{'loss': 0.0758, 'learning_rate': 7.63959955506118e-05, 'epoch': 3.09}\n",
      "{'loss': 0.0541, 'learning_rate': 7.595105672969966e-05, 'epoch': 3.1}\n",
      "{'loss': 0.072, 'learning_rate': 7.550611790878754e-05, 'epoch': 3.11}\n",
      "{'loss': 0.036, 'learning_rate': 7.506117908787542e-05, 'epoch': 3.13}\n",
      "{'loss': 0.0737, 'learning_rate': 7.46162402669633e-05, 'epoch': 3.14}\n",
      "{'loss': 0.0534, 'learning_rate': 7.417130144605116e-05, 'epoch': 3.15}\n",
      "{'loss': 0.0492, 'learning_rate': 7.372636262513904e-05, 'epoch': 3.16}\n",
      "{'loss': 0.0515, 'learning_rate': 7.328142380422692e-05, 'epoch': 3.17}\n",
      "{'loss': 0.077, 'learning_rate': 7.28364849833148e-05, 'epoch': 3.18}\n",
      "{'loss': 0.0462, 'learning_rate': 7.239154616240266e-05, 'epoch': 3.19}\n",
      "{'loss': 0.0499, 'learning_rate': 7.194660734149054e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0557, 'learning_rate': 7.150166852057842e-05, 'epoch': 3.21}\n",
      "{'loss': 0.0979, 'learning_rate': 7.10567296996663e-05, 'epoch': 3.23}\n",
      "{'loss': 0.0519, 'learning_rate': 7.061179087875418e-05, 'epoch': 3.24}\n",
      "{'loss': 0.0664, 'learning_rate': 7.016685205784205e-05, 'epoch': 3.25}\n",
      "{'loss': 0.07, 'learning_rate': 6.972191323692993e-05, 'epoch': 3.26}\n",
      "{'loss': 0.0597, 'learning_rate': 6.927697441601781e-05, 'epoch': 3.27}\n",
      "{'loss': 0.0333, 'learning_rate': 6.883203559510569e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0552, 'learning_rate': 6.838709677419355e-05, 'epoch': 3.29}\n",
      "{'loss': 0.0617, 'learning_rate': 6.794215795328143e-05, 'epoch': 3.3}\n",
      "{'loss': 0.0538, 'learning_rate': 6.749721913236931e-05, 'epoch': 3.31}\n",
      "{'loss': 0.0454, 'learning_rate': 6.705228031145719e-05, 'epoch': 3.33}\n",
      "{'loss': 0.0615, 'learning_rate': 6.660734149054505e-05, 'epoch': 3.34}\n",
      "{'loss': 0.0627, 'learning_rate': 6.616240266963293e-05, 'epoch': 3.35}\n",
      "{'loss': 0.0477, 'learning_rate': 6.571746384872081e-05, 'epoch': 3.36}\n",
      "{'loss': 0.0564, 'learning_rate': 6.527252502780869e-05, 'epoch': 3.37}\n",
      "{'loss': 0.059, 'learning_rate': 6.482758620689655e-05, 'epoch': 3.38}\n",
      "{'loss': 0.0548, 'learning_rate': 6.438264738598443e-05, 'epoch': 3.39}\n",
      "{'loss': 0.0551, 'learning_rate': 6.39377085650723e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0544, 'learning_rate': 6.349276974416018e-05, 'epoch': 3.41}\n",
      "{'loss': 0.044, 'learning_rate': 6.304783092324805e-05, 'epoch': 3.43}\n",
      "{'loss': 0.0672, 'learning_rate': 6.260289210233593e-05, 'epoch': 3.44}\n",
      "{'loss': 0.0625, 'learning_rate': 6.21579532814238e-05, 'epoch': 3.45}\n",
      "{'loss': 0.0753, 'learning_rate': 6.171301446051168e-05, 'epoch': 3.46}\n",
      "{'loss': 0.0595, 'learning_rate': 6.126807563959955e-05, 'epoch': 3.47}\n",
      "{'loss': 0.046, 'learning_rate': 6.082313681868743e-05, 'epoch': 3.48}\n",
      "{'loss': 0.0501, 'learning_rate': 6.037819799777531e-05, 'epoch': 3.49}\n",
      "{'loss': 0.0484, 'learning_rate': 5.993325917686319e-05, 'epoch': 3.5}\n",
      "{'loss': 0.0636, 'learning_rate': 5.948832035595105e-05, 'epoch': 3.52}\n",
      "{'loss': 0.0727, 'learning_rate': 5.904338153503893e-05, 'epoch': 3.53}\n",
      "{'loss': 0.0606, 'learning_rate': 5.859844271412681e-05, 'epoch': 3.54}\n",
      "{'loss': 0.0534, 'learning_rate': 5.815350389321469e-05, 'epoch': 3.55}\n",
      "{'loss': 0.0765, 'learning_rate': 5.770856507230256e-05, 'epoch': 3.56}\n",
      "{'loss': 0.0586, 'learning_rate': 5.726362625139044e-05, 'epoch': 3.57}\n",
      "{'loss': 0.0502, 'learning_rate': 5.6818687430478315e-05, 'epoch': 3.58}\n",
      "{'loss': 0.0553, 'learning_rate': 5.637374860956619e-05, 'epoch': 3.59}\n",
      "{'loss': 0.0422, 'learning_rate': 5.592880978865406e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0688, 'learning_rate': 5.5483870967741936e-05, 'epoch': 3.62}\n",
      "{'loss': 0.0471, 'learning_rate': 5.5038932146829814e-05, 'epoch': 3.63}\n",
      "{'loss': 0.055, 'learning_rate': 5.459399332591769e-05, 'epoch': 3.64}\n",
      "{'loss': 0.084, 'learning_rate': 5.414905450500557e-05, 'epoch': 3.65}\n",
      "{'loss': 0.052, 'learning_rate': 5.3704115684093435e-05, 'epoch': 3.66}\n",
      "{'loss': 0.0578, 'learning_rate': 5.325917686318131e-05, 'epoch': 3.67}\n",
      "{'loss': 0.0415, 'learning_rate': 5.281423804226919e-05, 'epoch': 3.68}\n",
      "{'loss': 0.0616, 'learning_rate': 5.236929922135707e-05, 'epoch': 3.69}\n",
      "{'loss': 0.0508, 'learning_rate': 5.192436040044494e-05, 'epoch': 3.7}\n",
      "{'loss': 0.0531, 'learning_rate': 5.147942157953282e-05, 'epoch': 3.72}\n",
      "{'loss': 0.0464, 'learning_rate': 5.10344827586207e-05, 'epoch': 3.73}\n",
      "{'loss': 0.0597, 'learning_rate': 5.0589543937708575e-05, 'epoch': 3.74}\n",
      "{'loss': 0.0519, 'learning_rate': 5.014460511679644e-05, 'epoch': 3.75}\n",
      "{'loss': 0.0704, 'learning_rate': 4.969966629588432e-05, 'epoch': 3.76}\n",
      "{'loss': 0.0518, 'learning_rate': 4.9254727474972196e-05, 'epoch': 3.77}\n",
      "{'loss': 0.051, 'learning_rate': 4.880978865406007e-05, 'epoch': 3.78}\n",
      "{'loss': 0.0746, 'learning_rate': 4.8364849833147945e-05, 'epoch': 3.79}\n",
      "{'loss': 0.0536, 'learning_rate': 4.7919911012235816e-05, 'epoch': 3.8}\n",
      "{'loss': 0.0721, 'learning_rate': 4.7474972191323694e-05, 'epoch': 3.82}\n",
      "{'loss': 0.055, 'learning_rate': 4.703003337041157e-05, 'epoch': 3.83}\n",
      "{'loss': 0.0531, 'learning_rate': 4.658509454949945e-05, 'epoch': 3.84}\n",
      "{'loss': 0.0482, 'learning_rate': 4.614015572858732e-05, 'epoch': 3.85}\n",
      "{'loss': 0.0708, 'learning_rate': 4.56952169076752e-05, 'epoch': 3.86}\n",
      "{'loss': 0.0563, 'learning_rate': 4.525027808676307e-05, 'epoch': 3.87}\n",
      "{'loss': 0.0539, 'learning_rate': 4.480533926585095e-05, 'epoch': 3.88}\n",
      "{'loss': 0.0651, 'learning_rate': 4.436040044493882e-05, 'epoch': 3.89}\n",
      "{'loss': 0.075, 'learning_rate': 4.39154616240267e-05, 'epoch': 3.9}\n",
      "{'loss': 0.0487, 'learning_rate': 4.347052280311457e-05, 'epoch': 3.92}\n",
      "{'loss': 0.0474, 'learning_rate': 4.302558398220245e-05, 'epoch': 3.93}\n",
      "{'loss': 0.0839, 'learning_rate': 4.258064516129032e-05, 'epoch': 3.94}\n",
      "{'loss': 0.0416, 'learning_rate': 4.21357063403782e-05, 'epoch': 3.95}\n",
      "{'loss': 0.0453, 'learning_rate': 4.1690767519466076e-05, 'epoch': 3.96}\n",
      "{'loss': 0.0381, 'learning_rate': 4.1245828698553954e-05, 'epoch': 3.97}\n",
      "{'loss': 0.063, 'learning_rate': 4.0800889877641825e-05, 'epoch': 3.98}\n",
      "{'loss': 0.0517, 'learning_rate': 4.0355951056729703e-05, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c0418c9d2a4d908b65254d995583fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª compute_metrics called\n",
      "{'eval_RMSE': 0.23712413012981415, 'eval_loss': 0.05622785538434982, 'eval_runtime': 22.806, 'eval_samples_per_second': 17.539, 'eval_steps_per_second': 4.385, 'epoch': 4.0}\n",
      "{'loss': 0.0524, 'learning_rate': 3.991101223581758e-05, 'epoch': 4.0}\n",
      "{'loss': 0.0374, 'learning_rate': 3.946607341490545e-05, 'epoch': 4.02}\n",
      "{'loss': 0.0442, 'learning_rate': 3.902113459399333e-05, 'epoch': 4.03}\n",
      "{'loss': 0.0454, 'learning_rate': 3.85761957730812e-05, 'epoch': 4.04}\n",
      "{'loss': 0.0932, 'learning_rate': 3.813125695216908e-05, 'epoch': 4.05}\n",
      "{'loss': 0.0619, 'learning_rate': 3.768631813125695e-05, 'epoch': 4.06}\n",
      "{'loss': 0.0506, 'learning_rate': 3.724137931034483e-05, 'epoch': 4.07}\n",
      "{'loss': 0.0766, 'learning_rate': 3.67964404894327e-05, 'epoch': 4.08}\n",
      "{'loss': 0.0333, 'learning_rate': 3.635150166852058e-05, 'epoch': 4.09}\n",
      "{'loss': 0.0614, 'learning_rate': 3.590656284760846e-05, 'epoch': 4.1}\n",
      "{'loss': 0.0339, 'learning_rate': 3.5461624026696336e-05, 'epoch': 4.12}\n",
      "{'loss': 0.0376, 'learning_rate': 3.501668520578421e-05, 'epoch': 4.13}\n",
      "{'loss': 0.0591, 'learning_rate': 3.4571746384872085e-05, 'epoch': 4.14}\n",
      "{'loss': 0.044, 'learning_rate': 3.4126807563959956e-05, 'epoch': 4.15}\n",
      "{'loss': 0.0546, 'learning_rate': 3.3681868743047834e-05, 'epoch': 4.16}\n",
      "{'loss': 0.0556, 'learning_rate': 3.3236929922135706e-05, 'epoch': 4.17}\n",
      "{'loss': 0.0702, 'learning_rate': 3.2791991101223584e-05, 'epoch': 4.18}\n",
      "{'loss': 0.0544, 'learning_rate': 3.2347052280311455e-05, 'epoch': 4.19}\n",
      "{'loss': 0.0458, 'learning_rate': 3.190211345939933e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0535, 'learning_rate': 3.1457174638487205e-05, 'epoch': 4.22}\n",
      "{'loss': 0.0594, 'learning_rate': 3.101223581757508e-05, 'epoch': 4.23}\n",
      "{'loss': 0.0412, 'learning_rate': 3.056729699666296e-05, 'epoch': 4.24}\n",
      "{'loss': 0.0523, 'learning_rate': 3.0122358175750836e-05, 'epoch': 4.25}\n",
      "{'loss': 0.0577, 'learning_rate': 2.967741935483871e-05, 'epoch': 4.26}\n",
      "{'loss': 0.0492, 'learning_rate': 2.923248053392659e-05, 'epoch': 4.27}\n",
      "{'loss': 0.0532, 'learning_rate': 2.878754171301446e-05, 'epoch': 4.28}\n",
      "{'loss': 0.0406, 'learning_rate': 2.8342602892102338e-05, 'epoch': 4.29}\n",
      "{'loss': 0.0444, 'learning_rate': 2.789766407119021e-05, 'epoch': 4.3}\n",
      "{'loss': 0.05, 'learning_rate': 2.7452725250278087e-05, 'epoch': 4.32}\n",
      "{'loss': 0.0529, 'learning_rate': 2.7007786429365962e-05, 'epoch': 4.33}\n",
      "{'loss': 0.0466, 'learning_rate': 2.656284760845384e-05, 'epoch': 4.34}\n",
      "{'loss': 0.0528, 'learning_rate': 2.611790878754171e-05, 'epoch': 4.35}\n",
      "{'loss': 0.062, 'learning_rate': 2.567296996662959e-05, 'epoch': 4.36}\n",
      "{'loss': 0.0551, 'learning_rate': 2.522803114571746e-05, 'epoch': 4.37}\n",
      "{'loss': 0.0532, 'learning_rate': 2.478309232480534e-05, 'epoch': 4.38}\n",
      "{'loss': 0.0567, 'learning_rate': 2.4338153503893217e-05, 'epoch': 4.39}\n",
      "{'loss': 0.0792, 'learning_rate': 2.3893214682981092e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0499, 'learning_rate': 2.3448275862068967e-05, 'epoch': 4.42}\n",
      "{'loss': 0.0764, 'learning_rate': 2.300333704115684e-05, 'epoch': 4.43}\n",
      "{'loss': 0.0463, 'learning_rate': 2.2558398220244716e-05, 'epoch': 4.44}\n",
      "{'loss': 0.0342, 'learning_rate': 2.211345939933259e-05, 'epoch': 4.45}\n",
      "{'loss': 0.0484, 'learning_rate': 2.166852057842047e-05, 'epoch': 4.46}\n",
      "{'loss': 0.0376, 'learning_rate': 2.1223581757508344e-05, 'epoch': 4.47}\n",
      "{'loss': 0.0555, 'learning_rate': 2.077864293659622e-05, 'epoch': 4.48}\n",
      "{'loss': 0.0607, 'learning_rate': 2.0333704115684093e-05, 'epoch': 4.49}\n",
      "{'loss': 0.0432, 'learning_rate': 1.9933259176863182e-05, 'epoch': 4.51}\n",
      "{'loss': 0.0466, 'learning_rate': 1.9488320355951057e-05, 'epoch': 4.52}\n",
      "{'loss': 0.0458, 'learning_rate': 1.9043381535038935e-05, 'epoch': 4.53}\n",
      "{'loss': 0.0589, 'learning_rate': 1.859844271412681e-05, 'epoch': 4.54}\n",
      "{'loss': 0.0792, 'learning_rate': 1.8153503893214684e-05, 'epoch': 4.55}\n",
      "{'loss': 0.0433, 'learning_rate': 1.770856507230256e-05, 'epoch': 4.56}\n",
      "{'loss': 0.043, 'learning_rate': 1.7263626251390437e-05, 'epoch': 4.57}\n",
      "{'loss': 0.0604, 'learning_rate': 1.6818687430478312e-05, 'epoch': 4.58}\n",
      "{'loss': 0.0563, 'learning_rate': 1.6373748609566187e-05, 'epoch': 4.59}\n",
      "{'loss': 0.0529, 'learning_rate': 1.592880978865406e-05, 'epoch': 4.61}\n",
      "{'loss': 0.0473, 'learning_rate': 1.5483870967741936e-05, 'epoch': 4.62}\n",
      "{'loss': 0.0313, 'learning_rate': 1.5038932146829812e-05, 'epoch': 4.63}\n",
      "{'loss': 0.0587, 'learning_rate': 1.4593993325917687e-05, 'epoch': 4.64}\n",
      "{'loss': 0.0679, 'learning_rate': 1.4149054505005564e-05, 'epoch': 4.65}\n",
      "{'loss': 0.0484, 'learning_rate': 1.3704115684093438e-05, 'epoch': 4.66}\n",
      "{'loss': 0.0486, 'learning_rate': 1.3259176863181313e-05, 'epoch': 4.67}\n",
      "{'loss': 0.0501, 'learning_rate': 1.281423804226919e-05, 'epoch': 4.68}\n",
      "{'loss': 0.0494, 'learning_rate': 1.2369299221357064e-05, 'epoch': 4.69}\n",
      "{'loss': 0.0459, 'learning_rate': 1.1924360400444939e-05, 'epoch': 4.71}\n",
      "{'loss': 0.0672, 'learning_rate': 1.1479421579532815e-05, 'epoch': 4.72}\n",
      "{'loss': 0.0694, 'learning_rate': 1.103448275862069e-05, 'epoch': 4.73}\n",
      "{'loss': 0.0606, 'learning_rate': 1.0589543937708565e-05, 'epoch': 4.74}\n",
      "{'loss': 0.0572, 'learning_rate': 1.0144605116796441e-05, 'epoch': 4.75}\n",
      "{'loss': 0.0425, 'learning_rate': 9.699666295884316e-06, 'epoch': 4.76}\n",
      "{'loss': 0.0684, 'learning_rate': 9.25472747497219e-06, 'epoch': 4.77}\n",
      "{'loss': 0.0743, 'learning_rate': 8.809788654060067e-06, 'epoch': 4.78}\n",
      "{'loss': 0.0415, 'learning_rate': 8.364849833147942e-06, 'epoch': 4.79}\n",
      "{'loss': 0.0756, 'learning_rate': 7.919911012235818e-06, 'epoch': 4.81}\n",
      "{'loss': 0.0499, 'learning_rate': 7.474972191323694e-06, 'epoch': 4.82}\n",
      "{'loss': 0.0391, 'learning_rate': 7.030033370411569e-06, 'epoch': 4.83}\n",
      "{'loss': 0.0483, 'learning_rate': 6.585094549499445e-06, 'epoch': 4.84}\n",
      "{'loss': 0.0344, 'learning_rate': 6.1401557285873195e-06, 'epoch': 4.85}\n",
      "{'loss': 0.08, 'learning_rate': 5.695216907675195e-06, 'epoch': 4.86}\n",
      "{'loss': 0.0572, 'learning_rate': 5.250278086763071e-06, 'epoch': 4.87}\n",
      "{'loss': 0.0532, 'learning_rate': 4.805339265850945e-06, 'epoch': 4.88}\n",
      "{'loss': 0.0614, 'learning_rate': 4.360400444938821e-06, 'epoch': 4.89}\n",
      "{'loss': 0.0613, 'learning_rate': 3.9154616240266965e-06, 'epoch': 4.91}\n",
      "{'loss': 0.0475, 'learning_rate': 3.4705228031145717e-06, 'epoch': 4.92}\n",
      "{'loss': 0.0664, 'learning_rate': 3.0255839822024472e-06, 'epoch': 4.93}\n",
      "{'loss': 0.0678, 'learning_rate': 2.580645161290323e-06, 'epoch': 4.94}\n",
      "{'loss': 0.0704, 'learning_rate': 2.135706340378198e-06, 'epoch': 4.95}\n",
      "{'loss': 0.0587, 'learning_rate': 1.6907675194660733e-06, 'epoch': 4.96}\n",
      "{'loss': 0.0467, 'learning_rate': 1.2458286985539489e-06, 'epoch': 4.97}\n",
      "{'loss': 0.0642, 'learning_rate': 8.008898776418243e-07, 'epoch': 4.98}\n",
      "{'loss': 0.0444, 'learning_rate': 3.559510567296997e-07, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2d667bfd2e4e7dbfd8ff4ac62d958c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª compute_metrics called\n",
      "{'eval_RMSE': 0.23862318694591522, 'eval_loss': 0.05694102868437767, 'eval_runtime': 15.5655, 'eval_samples_per_second': 25.698, 'eval_steps_per_second': 6.424, 'epoch': 5.0}\n",
      "{'train_runtime': 2264.1889, 'train_samples_per_second': 7.934, 'train_steps_per_second': 1.985, 'train_loss': 0.06705738789711699, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4495, training_loss=0.06705738789711699, metrics={'train_runtime': 2264.1889, 'train_samples_per_second': 7.934, 'train_steps_per_second': 1.985, 'train_loss': 0.06705738789711699, 'epoch': 5.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === 5. Training setup ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora_v2_5_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    metric_for_best_model=\"eval_RMSE\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(\"ðŸ§ª compute_metrics called\")\n",
    "    predictions, labels = eval_pred\n",
    "    mse = np.mean((predictions - labels) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return {\"eval_RMSE\": rmse}\n",
    "\n",
    "class RegressionTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        predictions = model(**inputs)\n",
    "        # weighted loss\n",
    "        loss = (\n",
    "            0.2 * mse_loss(predictions[:, 0], labels[:, 0]) +\n",
    "            0.2 * mse_loss(predictions[:, 1], labels[:, 1]) +\n",
    "            0.2 * mse_loss(predictions[:, 2], labels[:, 2]) +\n",
    "            0.2 * mse_loss(predictions[:, 3], labels[:, 3]) +\n",
    "            0.4 * mse_loss(predictions[:, 4], labels[:, 4])\n",
    "        )\n",
    "        output = SimpleNamespace(logits=predictions)\n",
    "        return (loss, output) if return_outputs else loss\n",
    "\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only=False, ignore_keys=None):\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        with torch.no_grad():\n",
    "            labels = inputs.pop(\"labels\") if has_labels else None\n",
    "            predictions = model(**inputs)\n",
    "            loss = mse_loss(predictions, labels) if has_labels else None\n",
    "        return (loss, predictions, labels)\n",
    "\n",
    "trainer = RegressionTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f9c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved to: lora_v2_5_model\n"
     ]
    }
   ],
   "source": [
    "# === 5.5 Save model ===\n",
    "save_path = \"lora_v2_5_model\"\n",
    "\n",
    "# Save LoRA adapter weights\n",
    "torch.save(model.backbone.state_dict(), f\"{save_path}/lora_adapter.bin\")\n",
    "\n",
    "# Save regression layer weights\n",
    "torch.save(model.regressor.state_dict(), f\"{save_path}/regression_head.pt\")\n",
    "\n",
    "# Save tokenizer and base model config\n",
    "tokenizer.save_pretrained(save_path)\n",
    "model.backbone.base_model.save_pretrained(save_path)\n",
    "\n",
    "# Save regression config (manual)\n",
    "reg_config = {\"hidden_size\": model.regressor.in_features, \"output_size\": model.regressor.out_features}\n",
    "torch.save(reg_config, f\"{save_path}/regression_config.pt\")\n",
    "\n",
    "print(\"âœ… Model saved to:\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8abd856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at lora_v2_5_model were not used when initializing DebertaV2Model: ['encoder.layer.0.attention.self.key_proj.base_layer.bias', 'encoder.layer.0.attention.self.key_proj.base_layer.weight', 'encoder.layer.0.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.0.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.0.attention.self.query_proj.base_layer.bias', 'encoder.layer.0.attention.self.query_proj.base_layer.weight', 'encoder.layer.0.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.0.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.0.attention.self.value_proj.base_layer.bias', 'encoder.layer.0.attention.self.value_proj.base_layer.weight', 'encoder.layer.0.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.0.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.1.attention.self.key_proj.base_layer.bias', 'encoder.layer.1.attention.self.key_proj.base_layer.weight', 'encoder.layer.1.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.1.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.1.attention.self.query_proj.base_layer.bias', 'encoder.layer.1.attention.self.query_proj.base_layer.weight', 'encoder.layer.1.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.1.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.1.attention.self.value_proj.base_layer.bias', 'encoder.layer.1.attention.self.value_proj.base_layer.weight', 'encoder.layer.1.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.1.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.10.attention.self.key_proj.base_layer.bias', 'encoder.layer.10.attention.self.key_proj.base_layer.weight', 'encoder.layer.10.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.10.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.10.attention.self.query_proj.base_layer.bias', 'encoder.layer.10.attention.self.query_proj.base_layer.weight', 'encoder.layer.10.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.10.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.10.attention.self.value_proj.base_layer.bias', 'encoder.layer.10.attention.self.value_proj.base_layer.weight', 'encoder.layer.10.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.10.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.11.attention.self.key_proj.base_layer.bias', 'encoder.layer.11.attention.self.key_proj.base_layer.weight', 'encoder.layer.11.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.11.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.11.attention.self.query_proj.base_layer.bias', 'encoder.layer.11.attention.self.query_proj.base_layer.weight', 'encoder.layer.11.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.11.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.11.attention.self.value_proj.base_layer.bias', 'encoder.layer.11.attention.self.value_proj.base_layer.weight', 'encoder.layer.11.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.11.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.2.attention.self.key_proj.base_layer.bias', 'encoder.layer.2.attention.self.key_proj.base_layer.weight', 'encoder.layer.2.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.2.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.2.attention.self.query_proj.base_layer.bias', 'encoder.layer.2.attention.self.query_proj.base_layer.weight', 'encoder.layer.2.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.2.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.2.attention.self.value_proj.base_layer.bias', 'encoder.layer.2.attention.self.value_proj.base_layer.weight', 'encoder.layer.2.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.2.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.3.attention.self.key_proj.base_layer.bias', 'encoder.layer.3.attention.self.key_proj.base_layer.weight', 'encoder.layer.3.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.3.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.3.attention.self.query_proj.base_layer.bias', 'encoder.layer.3.attention.self.query_proj.base_layer.weight', 'encoder.layer.3.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.3.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.3.attention.self.value_proj.base_layer.bias', 'encoder.layer.3.attention.self.value_proj.base_layer.weight', 'encoder.layer.3.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.3.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.4.attention.self.key_proj.base_layer.bias', 'encoder.layer.4.attention.self.key_proj.base_layer.weight', 'encoder.layer.4.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.4.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.4.attention.self.query_proj.base_layer.bias', 'encoder.layer.4.attention.self.query_proj.base_layer.weight', 'encoder.layer.4.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.4.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.4.attention.self.value_proj.base_layer.bias', 'encoder.layer.4.attention.self.value_proj.base_layer.weight', 'encoder.layer.4.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.4.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.5.attention.self.key_proj.base_layer.bias', 'encoder.layer.5.attention.self.key_proj.base_layer.weight', 'encoder.layer.5.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.5.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.5.attention.self.query_proj.base_layer.bias', 'encoder.layer.5.attention.self.query_proj.base_layer.weight', 'encoder.layer.5.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.5.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.5.attention.self.value_proj.base_layer.bias', 'encoder.layer.5.attention.self.value_proj.base_layer.weight', 'encoder.layer.5.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.5.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.6.attention.self.key_proj.base_layer.bias', 'encoder.layer.6.attention.self.key_proj.base_layer.weight', 'encoder.layer.6.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.6.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.6.attention.self.query_proj.base_layer.bias', 'encoder.layer.6.attention.self.query_proj.base_layer.weight', 'encoder.layer.6.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.6.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.6.attention.self.value_proj.base_layer.bias', 'encoder.layer.6.attention.self.value_proj.base_layer.weight', 'encoder.layer.6.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.6.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.7.attention.self.key_proj.base_layer.bias', 'encoder.layer.7.attention.self.key_proj.base_layer.weight', 'encoder.layer.7.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.7.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.7.attention.self.query_proj.base_layer.bias', 'encoder.layer.7.attention.self.query_proj.base_layer.weight', 'encoder.layer.7.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.7.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.7.attention.self.value_proj.base_layer.bias', 'encoder.layer.7.attention.self.value_proj.base_layer.weight', 'encoder.layer.7.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.7.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.8.attention.self.key_proj.base_layer.bias', 'encoder.layer.8.attention.self.key_proj.base_layer.weight', 'encoder.layer.8.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.8.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.8.attention.self.query_proj.base_layer.bias', 'encoder.layer.8.attention.self.query_proj.base_layer.weight', 'encoder.layer.8.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.8.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.8.attention.self.value_proj.base_layer.bias', 'encoder.layer.8.attention.self.value_proj.base_layer.weight', 'encoder.layer.8.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.8.attention.self.value_proj.lora_B.default.weight', 'encoder.layer.9.attention.self.key_proj.base_layer.bias', 'encoder.layer.9.attention.self.key_proj.base_layer.weight', 'encoder.layer.9.attention.self.key_proj.lora_A.default.weight', 'encoder.layer.9.attention.self.key_proj.lora_B.default.weight', 'encoder.layer.9.attention.self.query_proj.base_layer.bias', 'encoder.layer.9.attention.self.query_proj.base_layer.weight', 'encoder.layer.9.attention.self.query_proj.lora_A.default.weight', 'encoder.layer.9.attention.self.query_proj.lora_B.default.weight', 'encoder.layer.9.attention.self.value_proj.base_layer.bias', 'encoder.layer.9.attention.self.value_proj.base_layer.weight', 'encoder.layer.9.attention.self.value_proj.lora_A.default.weight', 'encoder.layer.9.attention.self.value_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2Model were not initialized from the model checkpoint at lora_v2_5_model and are newly initialized: ['encoder.layer.0.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.key_proj.weight', 'encoder.layer.0.attention.self.query_proj.bias', 'encoder.layer.0.attention.self.query_proj.weight', 'encoder.layer.0.attention.self.value_proj.bias', 'encoder.layer.0.attention.self.value_proj.weight', 'encoder.layer.1.attention.self.key_proj.bias', 'encoder.layer.1.attention.self.key_proj.weight', 'encoder.layer.1.attention.self.query_proj.bias', 'encoder.layer.1.attention.self.query_proj.weight', 'encoder.layer.1.attention.self.value_proj.bias', 'encoder.layer.1.attention.self.value_proj.weight', 'encoder.layer.10.attention.self.key_proj.bias', 'encoder.layer.10.attention.self.key_proj.weight', 'encoder.layer.10.attention.self.query_proj.bias', 'encoder.layer.10.attention.self.query_proj.weight', 'encoder.layer.10.attention.self.value_proj.bias', 'encoder.layer.10.attention.self.value_proj.weight', 'encoder.layer.11.attention.self.key_proj.bias', 'encoder.layer.11.attention.self.key_proj.weight', 'encoder.layer.11.attention.self.query_proj.bias', 'encoder.layer.11.attention.self.query_proj.weight', 'encoder.layer.11.attention.self.value_proj.bias', 'encoder.layer.11.attention.self.value_proj.weight', 'encoder.layer.2.attention.self.key_proj.bias', 'encoder.layer.2.attention.self.key_proj.weight', 'encoder.layer.2.attention.self.query_proj.bias', 'encoder.layer.2.attention.self.query_proj.weight', 'encoder.layer.2.attention.self.value_proj.bias', 'encoder.layer.2.attention.self.value_proj.weight', 'encoder.layer.3.attention.self.key_proj.bias', 'encoder.layer.3.attention.self.key_proj.weight', 'encoder.layer.3.attention.self.query_proj.bias', 'encoder.layer.3.attention.self.query_proj.weight', 'encoder.layer.3.attention.self.value_proj.bias', 'encoder.layer.3.attention.self.value_proj.weight', 'encoder.layer.4.attention.self.key_proj.bias', 'encoder.layer.4.attention.self.key_proj.weight', 'encoder.layer.4.attention.self.query_proj.bias', 'encoder.layer.4.attention.self.query_proj.weight', 'encoder.layer.4.attention.self.value_proj.bias', 'encoder.layer.4.attention.self.value_proj.weight', 'encoder.layer.5.attention.self.key_proj.bias', 'encoder.layer.5.attention.self.key_proj.weight', 'encoder.layer.5.attention.self.query_proj.bias', 'encoder.layer.5.attention.self.query_proj.weight', 'encoder.layer.5.attention.self.value_proj.bias', 'encoder.layer.5.attention.self.value_proj.weight', 'encoder.layer.6.attention.self.key_proj.bias', 'encoder.layer.6.attention.self.key_proj.weight', 'encoder.layer.6.attention.self.query_proj.bias', 'encoder.layer.6.attention.self.query_proj.weight', 'encoder.layer.6.attention.self.value_proj.bias', 'encoder.layer.6.attention.self.value_proj.weight', 'encoder.layer.7.attention.self.key_proj.bias', 'encoder.layer.7.attention.self.key_proj.weight', 'encoder.layer.7.attention.self.query_proj.bias', 'encoder.layer.7.attention.self.query_proj.weight', 'encoder.layer.7.attention.self.value_proj.bias', 'encoder.layer.7.attention.self.value_proj.weight', 'encoder.layer.8.attention.self.key_proj.bias', 'encoder.layer.8.attention.self.key_proj.weight', 'encoder.layer.8.attention.self.query_proj.bias', 'encoder.layer.8.attention.self.query_proj.weight', 'encoder.layer.8.attention.self.value_proj.bias', 'encoder.layer.8.attention.self.value_proj.weight', 'encoder.layer.9.attention.self.key_proj.bias', 'encoder.layer.9.attention.self.key_proj.weight', 'encoder.layer.9.attention.self.query_proj.bias', 'encoder.layer.9.attention.self.query_proj.weight', 'encoder.layer.9.attention.self.value_proj.bias', 'encoder.layer.9.attention.self.value_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# === 6.0 Load model for inference ===\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define model wrapper class again\n",
    "class DebertaWithLoRA(nn.Module):\n",
    "    def __init__(self, base_model, lora_config):\n",
    "        super().__init__()\n",
    "        self.backbone = get_peft_model(base_model, lora_config)\n",
    "        self.regressor = nn.Linear(base_model.config.hidden_size, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        if \"labels\" in kwargs:\n",
    "            kwargs.pop(\"labels\")\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        return self.regressor(cls_output)\n",
    "\n",
    "# Load components\n",
    "load_path = \"lora_v2_5_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "base_model = AutoModel.from_pretrained(load_path)\n",
    "\n",
    "# LoRA config must match training\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "\n",
    "# Build model again\n",
    "model = DebertaWithLoRA(base_model, lora_config)\n",
    "\n",
    "# Load weights\n",
    "model.backbone.load_state_dict(torch.load(os.path.join(load_path, \"lora_adapter.bin\")), strict=False)\n",
    "model.regressor.load_state_dict(torch.load(os.path.join(load_path, \"regression_head.pt\")))\n",
    "\n",
    "# Move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ… Model and tokenizer loaded for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7bca4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Inference ===\n",
    "def postprocess_scores(scores):\n",
    "    scores = scores * 5 + 4\n",
    "    scores = torch.round(scores * 2) / 2\n",
    "    return torch.clamp(scores, min=4.0, max=9.0)\n",
    "\n",
    "def predict_scores(prompt, essay, model, tokenizer):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    text = f\"Prompt: {prompt}\\nEssay: {essay}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).squeeze()\n",
    "        scores = postprocess_scores(outputs)\n",
    "\n",
    "    return {\n",
    "        \"Task Achievement\": scores[0].item(),\n",
    "        \"Coherence & Cohesion\": scores[1].item(),\n",
    "        \"Lexical Resource\": scores[2].item(),\n",
    "        \"Grammar\": scores[3].item(),\n",
    "        \"Overall Band\": scores[4].item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9f3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Task Achievement': 6.5, 'Coherence & Cohesion': 6.0, 'Lexical Resource': 6.0, 'Grammar': 6.0, 'Overall Band': 6.5}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Interviews form the basic selecting criteria for most large companies. However, some people think that the interview is not a reliable method of choosing whom to employ and there are other better methods. To what extent do you agree or disagree?\"\n",
    "\n",
    "# essay = (\"It is undeniable that most companies rely on interviews for hiring new employees, but some people believe that this dependence on interviews is wrong and that other alternatives should be preferred as they are better. I believe that interviews are a reliable method, and the other methods cannot be considered more effective.There are many reasons why interviews are suitable for hiring new employees. Firstly, through the interview, the recruiters can get an idea about the personality and social skills of the potential employees. In interviews, there is face-to-face interaction, and the candidates have to answer impromptu questions, from which personality traits can be judged. Also, by asking some case study-type questions, employees can judge traits like the ability to handle pressure, confidence and the ability to think outside the box.In addition, although there are many other methods for hiring, none of them could be considered superior to interviews. One the other common method is the written test, which is good to judge the theoretical knowledge of the person. However, this method cannot give a good idea about the personality of the candidate. By contrast, through interviews, employers can judge both the knowledge and temperament of the potential employee. Moreover, the written test is fallible to cheating as sometimes candidates can take outside help.Another selection process is a group discussion, which is good where a major job requirement is conversational and persuasion skills for instance sales jobs. They are really not suited for technical jobs because these are not customer oriented. On the other hand, interviews hold good for any type of job as the interviewer can frame questions to test the particular skills they require.To conclude, I would like to reiterate that the preference for interviews for hiring among employers is justified as interviews have advantages over other commonly adopted methods.\")\n",
    "# essay = (\"It is irrefuitable that most companies rely on interviews for hiring new employees , but some people opine that alternatives methods should be preferred as they are best as compared to the dependence on interview. I totally disagree with this statement as interview is best way to know about individual's potential and personality skills ,however, other methods such as written exam are is lengthy procedure .To begin with, there are much reasons why interviews are the best way. firstly, with the ,interviews an employer can get an idea about the personality and social skills of the employee. Because it is face to face interaction in which candidate have to answer different questions, from which personality traits can be judged by employer. Moreover, potential skills are vital for any organisation so by asking some case study question , employees can judge the ability to handle pressure , confidence, ability to think in an adverse situation . For instance, in the private ,sector all MNC totally depend upon direct interviews through which the organiser get a chance to know the mental ability of a person.On the other hand, some people opine that written test is the way of judgement they consider theoretical knowledge is important than practical knowledge. however, this way can not give a best idea about candidate's skills. Moreover, the written test is fallible to cheating as sometimes candidates can take outside help. Such as, in public sector recruitment government conduct exam in which scams take place and an unable person got a selection . So how anyone can consider this way best for hiring an employee .To sum up, in my opinion, interviews are justified as it has advantages over the commonly adopted methods.\")\n",
    "# essay = (\"Interviews are a common selection method for many companies, but they are not the only method. Other methods such as skills tests, reference checks, and work samples may provide a more accurate assessment of a candidate's abilities and fit for the job.While interviews can give an initial impression of a candidate, they may not provide a complete picture of their skills, personality, and work style. Interviews may also be subject to biases, such as unconscious biases or interviewer biases, which can affect the outcome of the selection process.Additionally, some candidates may perform well in an interview but not be a good fit for the job, while others may not perform as well in an interview but have the necessary skills and qualities for the job.Therefore, while interviews can be a useful selection method, they should not be relied on solely to make hiring decisions. Combining interviews with other selection methods may provide a more comprehensive assessment of candidates and increase the chances of making a successful hire.\")\n",
    "# essay = (\"It is believed by some experts that the interview is the main factor by which an employee should get a chance to work in an organization. However, Some are against this perception and think that this is not the only way of selecting an individual, there can be other reliable methods by which the workforce can be chosen. I agree with the latter view to some extent.There are myriad reasons why an employer finds an interview to be the best way among all other alternatives. Firstly, It includes face-to-face interaction with the person which can benefit the employer in understanding the real potential of people. Secondly, It is helpful in accessing the traits related to their personality, especially, their communication skills which are necessary for an employee to work in any department. In addition to it, an interviewer can judge the mental ability of the humans by providing them with the tasks related to their capability.Apart from this, It is also believed by companies that it assists them in hiring the right person in comparison to other competitive exams. For instance, It might be possible that a person with a good academic result is not able to perform a particular task, therefore, resulting in lower productivity.Furthermore, It is not possible to ensure the correctness of the details mentioned in the curriculum vitae by the candidate in other selection forms.Although It has various advantages which overweigh the disadvantages yet I believe that there should be a mix of both terminologies in order to appoint a worker in a company. There are some jobs which did not require personal skills, such as these are not mandatory for the post of a computer operator. Only written and theoretical knowledge is enough to fill the vacancy for this post.To sum up, I would like to reiterate that if I consider it logically then I believe there are some jobs for which there is no need to conduct an interview. So, it is wise to say that firms should use a combination of these terms depending upon the requirement of a particular position when selecting an employee.\")\n",
    "# essay = (\"Nowadays, employing citizens in a variety of companies by interviewing them is an impactful process. However, there are cultures that believe it doesn't reflect their true abilities. On the opposite, there are business owners who believe it is efficient to select people who stand out. In this essay, I will demonstrate why this method has a positive outcome in most cases for companies.Currently, people who think attending personal sessions in business areas has a negative outcome, are usually unemployed based on their experience. This is simply because they lack the skills and knowledge to stand out, thus It will be difficult to get hired. For instance, if a student had a long journey of only studying without having experience in the market, they won't find a job. This means although It is wrong to spread information about what human resources should do to hire. Ultimately, investing time to develop skills is a key factor to have a higher chance of occupation.However, most of our society believes it is an efficient method because it reflects behaviour and work ethic. Indeed, these factors are impactful in the work environment to avoid having lazy employees, thus others won't be productive as they were. For instance, if the employee does not invest enough effort in reaching the goals set for him, thus business owner will fire him in the long run. Without this ,we wouldn't know the truth of what is inside the workers. Clearly, using this is a must to have a better idea of how efficient he will be at work.In conclusion, I believe we should spread awareness to reduce the number of people who think it is not the right way to select employees. In addition, citizens should prepare themselves to go through this process rather than criticise it.\")\n",
    "essay = (\"Many multinational and national companies select their staff after a lot of research. In that face to face interaction is the prime site for an employee hiring. Although, it is considered by few that interview is not an appropriate way of filling the vacancy and there are more reliable sources. However, I mostly disagree with the view. My point is justified further.If it was easy to crack an interview of reputed establishments, then everyone might had got a place in each corporation. Moreover, it is not a piece of cake for all, it needs courage, dedication along with proper knowledge because all true colours flow out in front of interviewers. One of the many criteria judged is, crammed theory. For example, when a person is given a complicated situation and asked how will they put their learning into an action. Such is the time when the stars start surrounding around the head, because all they remember is what the theory says, but do not know the real life application of that intelligence. Thus, the individual gets rejected as they do not need a book, but a human engine to run their machines. Furthermore, other criteria to qualify is, communication skills. For example, if a product is needed to be sold, then the buyers should be provided with as best offer as possible, just for company profit. Whatever, the business it is running, the best way to buy or sell products is negotiation with the way of talking and persuading them with some schemes. In addition, introsceptiner also notes the practical exposure and change of facial express. For example, expressions like lies, fear, worry, anxiety and the capability of a man.The capability of a person is also judged by his past interest and achievements until now.On the contrary, the era has changed and continuation of age old methods of employee acceptance needs a change. This can be bought by replacing the system with either questionnaire pamphlet or multiple short tests. For example, all the interviewing questions, exam can be taken or else multiple different field exam. By this the need for interview will also not arise and the manager can fully check the person, along with saving their time.To conclude, the new system can be introduced, but interviews cannot be replaced by those methods, as it is the most reliable source for knowing a person thoroughly.\")\n",
    "\n",
    "print(predict_scores(prompt, essay, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea335a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Some people believe that eventually all jobs will be done by artificially intelligent robots. \\nWhat is your opinion?', 'essay': 'Some may argue that in the future all human workforces will be replaced by robots. While it is possible that most low skill jobs will be taken over by artificial intelligent computers, it is still difficult for some area of work to fully rely on them. This essay will discuss why in the end humans are still needed to do many other jobs.\\r\\n\\r\\nFirst, the majority of the non-complex occupation will be handle by robot in the future. It simply because, robot can be easily programmed to do a constant and simple job, moreover, companies will try their best to limit the salary expense by hiring more robots to replace the non-essential workforce. In Indonesia for example, they already started to create an automatic payment gate in every toll-roads. In addition, this automatic gate now being implemented and more common to be found as an exit gate in the parking area. For these reasons, ,eventually many low-level jobs will be replaced by robot in the future.\\r\\n\\r\\nOn the other hand, there are a lot of specific jobs that utterly cannot be substituted by another thing but human.  This condition applies because that kind of work often needs the intervere of the human brain as decision-makers. Doctors for instance, need to analyse many aspects of patients before taking actions and receipt of proper medicines. That kind of complex decision-making process is very unique and robot will not be able to compete with humans.\\r\\n\\r\\nIn conclusion, even though in the future robot might take over many kinds of job, there are still numerous jobs that will never be replaced by it, more specifically jobs that involving the decision-making process. Ultimately, the increasing number of robots as work labourers are an inevitability and only a few jobs will incompatible for them.', 'evaluation': '**Task Achievement:**\\n\\nThe candidate has effectively addressed the given task by presenting a clear opinion on the topic. They have discussed the potential of AI robots replacing human jobs in the future and provided relevant arguments to support their opinion. However, the essay lacks a clear structure and fails to cover all aspects of the task fully. The candidate could have provided more specific examples and evidence to strengthen their arguments.\\n\\n**Suggested Band Score (Task Achievement): 6.0**\\n\\n**Coherence and Cohesion:**\\n\\nThe essay lacks coherence and cohesion. The ideas are not presented in a logical sequence, and the transitions between sentences and paragraphs are not smooth. The candidate could use more connecting words and phrases to improve the overall flow of the essay.\\n\\n**Suggested Band Score (Coherence and Cohesion): 5.0**\\n\\n**Lexical Resource (Vocabulary):**\\n\\nThe candidate has used a limited range of vocabulary. The essay contains several repetitions of words, such as \"robot\" and \"job.\" The candidate could have used more diverse and sophisticated vocabulary to enhance the overall quality of the essay.\\n\\n**Suggested Band Score (Lexical Resource): 5.0**\\n\\n**Grammatical Range and Accuracy:**\\n\\nThe essay contains several grammatical errors, such as incorrect verb forms and sentence construction. For example, in the sentence \"This condition applies because that kind of work often needs the intervere of the human brain as decision-makers,\" the word \"intervere\" should be \"intervention.\" The candidate should proofread their work carefully to identify and correct grammatical errors.\\n\\n**Suggested Band Score (Grammatical Range and Accuracy): 5.0**\\n\\n**Overall Band Score:**\\n\\nConsidering the performance across all criteria, the essay is given an overall band score of **5.5**. The essay addresses the task, but it lacks coherence, cohesion, and a diverse range of vocabulary. The candidate should focus on improving their writing skills in these areas to achieve a higher band score.\\n\\n**Feedback and Additional Comments:**\\n\\n**Strengths:**\\n\\n- The essay presents a clear opinion on the topic.\\n- The candidate has provided some relevant arguments to support their view.\\n\\n**Areas for Improvement:**\\n\\n- Improve coherence and cohesion by using more connecting words and phrases and arranging the ideas in a logical sequence.\\n- Expand the range of vocabulary used in the essay.\\n- Proofread the essay carefully to identify and correct grammatical errors.\\n- Provide more specific examples and evidence to strengthen the arguments.', 'band': '5.5\\r\\r\\r', 'task_score': 6.0, 'coherence_score': 5.0, 'lexical_score': 5.0, 'grammar_score': 5.0, 'overall_score': 5.5}\n",
      "{'prompt': 'Some people believe that eventually all jobs will be done by artificially intelligent robots. What is your opinion?', 'essay': \"While it is true that in many nations of people have lots of engaged in travelling, because it has become a major part in their life. At present, more and more individuals liking travel from one place to another place than last few decades. There are many reasons behind this phenomenon. I deem that travelling has enough of advantages for the holiday makers. I will discuss the certain reasons and some advantages of travelling in the below paragraphs.\\n\\nTo start with there are two major factors which help to explain ideas why travelling is becoming more popular than in the past time. First reason is that people know very well, in the before time people had less interest in travelling, as they most liked to earn the money for fulfilling the needs of their family members. Because of this they had to work hard around the clock. Owing to this circumstance, they had no time for travelling. However, now the trend of this has varied in many ways. Technology has become faster, convenient, easier with the advents of current features. Namely transportation, which saves the time of the people. Nevertheless, people love travelling. Secondly, it has become enjoyable for their travellers. To illustrate, even if a person working in a big company at present. He or she gets promotions from their companies in order to go one place to another place either seven or a month with family, which the way of the this is an affordable. On the contrary side, it is impossible last few decades. Due to, people tend to travel in their life.\\n\\nIn spite of the causes, there are more advantages of departing for the passengers. First and foremost benefit is that it helps to provide the opportunity to make new friends. For instance, when a person moves to another country in life. As a result, he or she meets new friends, who belong to other traditions. From those  people he gets a chance to see their culture, conventional items of their country, lifestyles, foods and many more. In addition to it, travellers get an opportunity to have a national identity of their country, where they are able to live for a long time as well as learn about languages, which helps to stay connected with other people. It uses to enhance the in depth of knowledge for the layman.\\n\\nTo sum up, even though moving one region to another region has more reasons, yet while travelling has been many advantages for the trippers. I judge that it is playing a considerable role in everyone's life as well as very beneficial to increase the knowledge.\", 'evaluation': '## Task Achievement:\\n- The candidate has adequately addressed the given task by discussing the reasons and advantages of travelling becoming more popular.\\n- The ideas presented are clear and relevant to the topic.\\n- However, the response lacks sufficient depth and analysis, and some aspects of the task are not fully covered.\\n- The essay does not provide strong arguments or evidence to support the claims made.\\n- Suggested Band Score (Task Achievement): 6.5\\n\\n## Coherence and Cohesion:\\n- The essay lacks a clear and logical structure.\\n- Transitions between sentences and paragraphs are not smooth, and the flow of ideas is disjointed.\\n- Connecting words and phrases are used inconsistently, which hinders the overall coherence of the text.\\n- Suggested Band Score (Coherence and Cohesion): 5.5\\n\\n## Lexical Resource (Vocabulary):\\n- The vocabulary used in the essay is limited and repetitive.\\n- Some words are used inaccurately or in an inappropriate context.\\n- The essay lacks variety in sentence structures and complexity.\\n- Suggested Band Score (Lexical Resource): 5.5\\n\\n## Grammatical Range and Accuracy:\\n- The essay contains several grammatical errors, including incorrect verb forms, sentence construction, and punctuation.\\n- The use of tenses is inconsistent, and some sentences are incomplete or unclear.\\n- The essay lacks variety in sentence structures and complexity.\\n- Suggested Band Score (Grammatical Range and Accuracy): 5.0\\n\\n## Overall Band Score:\\n- Considering the holistic performance across all criteria, the overall band score for the essay is **8**.\\n- The essay demonstrates some strengths in task achievement, but the weaknesses in coherence and cohesion, lexical resource, and grammatical accuracy significantly impact the overall quality of the writing.\\n\\n## Feedback and Additional Comments:\\n\\n**Strengths**:\\n- The essay addresses the topic directly and provides some relevant ideas.\\n- The introduction and conclusion are generally clear and cohesive.\\n\\n**Areas for Improvement**:\\n- **Coherence and Cohesion**: Improve the flow of ideas by using appropriate transitions and connecting words. Ensure a logical sequence and arrangement of information throughout the essay.\\n- **Lexical Resource**: Expand vocabulary by using a wider range of words and phrases. Avoid repetition and use words accurately.\\n- **Grammatical Range and Accuracy**: Pay attention to grammatical accuracy and consistency. Use a variety of sentence structures and avoid errors in verb forms, sentence construction, and punctuation.\\n- **Task Achievement**: Provide more in-depth analysis and support for the claims made. Use specific examples or evidence to strengthen the arguments.\\n\\n**Overall**:\\nThe essay has potential but needs significant improvement in coherence, cohesion, lexical resource, and grammatical accuracy. By addressing these areas, the candidate can enhance the overall quality of their writing and achieve a higher band score.', 'band': '8\\r\\r\\r', 'task_score': 6.5, 'coherence_score': 5.5, 'lexical_score': 5.5, 'grammar_score': 5.0, 'overall_score': 8.0}\n",
      "{'prompt': 'The housing shortage in big cities can cause severe social consequences. Some people think only government action can solve this problem. To what extent do you agree or disagree with the above statement?', 'essay': 'Over the past few decades, the issue of housing shortage in an urban area has been a major concern because an expensive land caused serious problems in society. Although some people claim that only governments can deal with this matter, in my opinion with a combination both of the government and an individual, this problem can be tackled  as I will discuss in the following essay. \\n\\n\\nThere is no doubt that governments have the power to establish legislation and subsidies. For example, governments will fine real estate agencies if they overcharge people who buy houses in the city, Therefore, real estate agencies would provide a reasonable price. Furthermore, another thing governments can do is to provide financial supported who buy houses in the city . A good example of this is Taiwan authority share a proportion of money to subsidies people buying a housing in  urban areas. As a result, citizens can alleviate the burden of  financial concern. \\n\\n\\n\\nOn the other hand, an ndividual also need to take action in order to deal with this problem. People should try  to to find vertical apartments instead of  private houses in urban areas. For example,  In Singapore, the majority people tend to live in high-rise building so that more people can live in the city. By doing this,  the apartments can accommodate more population in the urban area. \\n\\n\\nIn conclusion, despite the fact that state should take more responsibility, from my perspective, a combination of authority and individuals should be coordinated to solve the problems. Provided benefits as well as are encouraged, people to live in urban areas, this phenomenon can be alleviated.', 'evaluation': '### Task Achievement:\\n- The candidate has adequately addressed the task by discussing the extent to which government action and individual action can solve the housing shortage in big cities.\\n- Ideas are presented clearly and are generally relevant to the task, but there are some instances where the connection to the task could be stronger.\\n- While most aspects of the task have been covered, some arguments could be supported with more detail and evidence.\\n- Overall, the candidate has fulfilled the requirements of the task, but there is room for improvement in providing more in-depth analysis and support for ideas.\\n- Suggested Band Score (Task Achievement): 6.5\\n\\n### Coherence and Cohesion:\\n- Transitions between sentences and paragraphs are generally clear and help maintain the flow of ideas.\\n- Connecting words and phrases are used effectively in most cases.\\n- The logical sequence of information is mostly maintained throughout the essay, but some paragraphs could be better organized to enhance the progression of ideas.\\n- The overall organization and structural integrity are adequate but could be improved with more careful planning and sequencing of paragraphs.\\n- Suggested Band Score (Coherence and Cohesion): 6.5\\n\\n### Lexical Resource (Vocabulary):\\n- The range of vocabulary used is generally good, but there are some instances of repetition and overuse of certain words, such as \"solve\" and \"problem.\"\\n- Mistakes in vocabulary include incorrect usage of \"an ndividual\" instead of \"an individual\" and \"are encouraged\" instead of \"encourage.\"\\n- The vocabulary is mostly appropriate for the given context, but there are a few instances where more precise or sophisticated vocabulary could be used.\\n- Suggested Band Score (Lexical Resource): 6.0\\n\\n### Grammatical Range and Accuracy:\\n- The variety of sentence structures is limited, with a predominance of simple sentences.\\n- Grammatical errors include incorrect verb forms, such as \"real estate agencies would provide\" instead of \"real estate agencies should provide\" and \"On the other hand, an ndividual also need to take action\" instead of \"On the other hand, individuals also need to take action.\"\\n- Punctuation and sentence formation are generally accurate, but there are a few minor errors, such as missing commas.\\n- Suggested Band Score (Grammatical Range and Accuracy): 6.0\\n\\n### Overall Band Score:\\n- Considering the holistic performance across all criteria, the overall band score for this essay is 6.5.\\n- The essay meets the task requirements and presents ideas in a coherent and cohesive manner.\\n- The range of vocabulary and grammatical structures is adequate, but there is room for improvement in accuracy and variety.\\n- The essay would benefit from more in-depth analysis, support for ideas, and careful planning of organization and sequencing.\\n\\n### Feedback and Additional Comments:\\n- Strengths:\\n    - Clear and direct addressing of the task.\\n    - Good use of connecting words and phrases.\\n    - Adequate use of examples to support ideas.\\n- Areas for Improvement:\\n    - Provide more in-depth analysis and support for ideas.\\n    - Enhance the variety and complexity of sentence structures.\\n    - Improve grammatical accuracy, particularly in verb forms.\\n    - Carefully plan the organization and sequencing of paragraphs to ensure a logical flow of ideas.\\n- Suggestions for Enhancement:\\n    - Use more specific and sophisticated vocabulary to convey ideas more precisely.\\n    - Proofread the essay carefully to identify and correct grammatical errors.\\n    - Seek feedback from others to gain different perspectives and identify areas for improvement.', 'band': '6.5\\n\\n\\r\\r\\r\\r\\r\\r\\r\\r\\r\\r', 'task_score': 6.5, 'coherence_score': 6.5, 'lexical_score': 6.0, 'grammar_score': 6.0, 'overall_score': 6.5}\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(dataset[\"train\"][i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
